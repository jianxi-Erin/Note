

# 模块A:操作系统基础环境

个人需要知道和理解：

```
 Linux 操作系统基本操作与常用的命令

 Linux 操作系统中的用户管理、权限管理

 文件和目录管理、文件的打包与压缩

 定时任务制定

 文件查找

 Linux 中管道与数据流重定向
```



个人应能够：

```
掌握 Linux 操作的基本使用

掌握用户管理、权限管理、文件管理的操作

掌握文件的打包与解压缩操作

掌握定时任务的创建与管理

掌握 awk、sed 等常用命令的使用
```



## 主要知识和技能点:

```
用户管理、权限管理、文件打包与解压缩、计划任务的管理、文件的查找、文件编辑、AWK 命令、SED 命令等
```

###### 一.用户管理

1. 添加用户

`sudo`是指以root用户执行命令,如已用root用户登录,`sudo`可以省略

要添加一个新用户，可以使用`useradd`命令，命令格式如下：

```
sudo useradd username
```

在这里，`username`是将要创建的用户的用户名。

`-m` 选项表示创建用户的同时创建其主目录(centos 7默认会自动创建主目录)。

`-M` 选项表示创建用户的同时不创建主目录

`-p`选项为指定用户的密码

`-g`指定用户组

例如：

```
sudo useradd -p password username
```

2. 删除用户

要删除一个现有的用户，可以使用`userdel`命令，命令格式如下：

```shell
#不会删除主目录
sudo userdel username
#同时删除主目录
sudo userdel -r username


#如果提示用户正在使用,可以用下面命令强制删除
#强制删除用户,即使用户已登录     #仍然会提提示用户正在使用,但其实已删除
sudo userdel -f username

```

在这里，`username`是要删除的用户的用户名。

3. 修改用户密码

要修改一个用户的密码，可以使用`passwd`命令，命令格式如下：

```
sudo passwd username
```

`username`是要修改密码的用户名。命令运行后会提示输入新的密码两次。

4. 修改用户信息

要修改一个用户的信息，可以使用`usermod`命令，命令格式如下：

```
sudo usermod [options] username
```

在这里，`options`是一些可用选项，例如:

`-c`选项来修改用户的注释

`-l`选项修改用户名(不修改主目录名)

`-d`选项修改用户主目录

`-g`选项修改用户组

`-aG`参数表示将用户添加到指定的辅助组中，如果该组不存在，则会创建该组。多个组名之间用逗号分隔。

*每个用户都属于至少一个组。除了主组之外，用户可以被分配到多个辅助组中。这些辅助组可以访问某些资源、文件或目录，而其他用户可能无法访问。*

例如:

```
sudo usermod -c "New User Comment" username
sudo usermod -l new_username old_username
sudo usermod -d /new/home/dir username
sudo usermod -g new_groupname username
sudo usermod -aG group1,group2,group3 username

```

5. 列出所有用户

查看当前登录用户信息

```shell
#用于显示当前用户或组的ID号
id
#显示当前的登录的用户名,终端id,登录时间和登录ip
who
#显示当前的用户名
whoami
```

要列出系统中的所有用户，可以使用以下命令：

```shell
cat /etc/passwd




#例如
root:x:0:0:root:/root:/bin/bash
#root：用户名。
#x：密码占位符，表示密码并不存储在 /etc/passwd 文件中，而是存储在 /etc/shadow 文件中。
#0：用户ID（UID），root 用户的 UID 为 0，是系统中唯一的超级用户。
#0：组ID（GID），root 用户所属的主组ID，通常也为 0。
#root：用户全名或注释，表示该用户的真实姓名或其他附加信息。
#/root：用户主目录，即该用户登录系统后要进入的目录。
#/bin/bash：登录Shell程序，即该用户登录系统后默认使用的Shell程序。
```

这会显示所有用户的详细信息。

6. 切换用户

要切换到另一个用户账户，可以使用`su`命令，命令格式如下：

```
su - username
```

在这里，`username`是要切换到其账户的用户名。如果没有指定任何用户名，则默认切换到root用户。



###### 二.权限管理

在CentOS系统中，权限管理主要包括用户管理和文件权限管理两个方面。下面分别介绍：

1. 用户管理：

CentOS系统中可以通过以下命令进行用户管理：

- 添加用户：`useradd 用户名`
- 删除用户：`userdel 用户名`
- 修改用户信息：`usermod 参数 用户名`

其中，参数可以包括：

- `-c`：添加用户说明
- `-d`：修改用户的家目录
- `-g`：修改用户的主组
- `-G`：修改用户的附加组
- `-s`：修改用户的默认Shell程序

2. 文件权限管理：

在CentOS系统中，每个文件或目录都包含了三种权限类型：<u>读、写、执行。这些权限对应的数字值为4、2、1。</u>通过将这些值相加，可以得到一组三位数的权限表示方式。

文件属主、文件所属组和其他用户分别对应了这三个数字。例如，<u>一个文件权限为654，表示文件所属者可读可写，文件所属组可读可执行,其他用户只能读取。</u>

CentOS系统中可以通过以下命令进行文件权限的管理：



- `chmod 权限 类型 文件名`：修改某个文件或目录的权限。其中，权限可以用<u>数字</u>或文字表示。

  数字表示法例如:

  1. 将文件`example.txt`的权限修改为<u>所有用户只能读取</u>，其他权限不变：

     ```
     chmod 444 example.txt
     ```

  2. 将文件`example.sh`的权限修改为<u>文件属主和用户组用户可读可写可执行</u>,<u>其他用户可读</u>，：

     ```
     chmod 774 example.sh
     ```

  3. 将目录`/var/www/html`及其下所有文件和子目录的权限修改为<u>文件属主用户可读写执行</u>,<u>所属用户组用户可读可执行</u>，<u>其他用户没有任何权限</u>：

     ```
     chmod -R 750 /var/www/html
     ```

  符号表示法例如:

  ```
  chmod [who] operator [permission] file
  ```

  其中，`who`表示要修改权限的用户或用户组，可以是以下值之一：

  - `u` 表示文件属主
  - `g` 表示文件所属群组
  - `o` 表示其他用户
  - `a` 表示所有用户（即`u g o`的合集）

  `operator`表示要对权限做出的操作，可以是以下值之一：

  - `+` 表示增加权限
  - `-` 表示移除权限
  - `=` 表示覆盖权限

  `permission`表示具体要修改的权限，可以是以下值之一：

  - `r` 表示读取权限
  - `w` 表示写入权限
  - `x` 表示执行权限

  下面举几个例子：

  1. 将文件`example.txt`的属主添加执行权限

  ```
  chmod u+x example.txt
  ```

  解释：`u`表示文件属主，`+`表示添加权限，`x`表示执行权限。

  2. 将文件`example.txt`的群组移除写入权限

  ```
  chmod g-w example.txt
  ```

  解释：`g`表示文件所属群组，`-`表示移除权限，`w`表示写入权限。

  3. 将文件`example.txt`的所有用户覆盖为只读权限

  ```
  chmod a=r example.txt
  ```

  解释：`a`表示所有用户，`=`表示覆盖权限，`r`表示读取权限。

  

- `chown 属主:所属组 文件名`：修改文件或目录的属主和所属组。

  

  `chown`命令的使用格式如下：

  ```
  chown [选项] [属主][:[所属群组]] 文件或目录
  ```

  其中，选项常用的有以下两个：

  - `-R` 递归地更改目录及其下所有文件的属主和所属群组。
  - `--reference=RFILE` 将指定文件的属主和所属群组复制到另一个文件或目录。

  `属主`表示要将该文件或目录的属主修改为指定的用户，可以是用户名或用户ID。如果省略`:[所属群组]`部分，则该文件或目录的所属群组不变。如果指定了`:[所属群组]`，则该文件或目录的所属群组也会被修改为指定的群组名或群组ID。

  下面是几个使用`chown`命令的例子:

  1. 将`file.txt`的属主修改为`user1`：

  ```
  chown user1 file.txt
  ```

  2. 将`file.txt`的属主修改为`user1`，所属群组修改为`group1`：

  ```
  chown user1:group1 file.txt
  ```

  3. 将目录`/home/dir1`及其下所有文件的属主修改为`user1`：

  ```
  chown -R user1 /home/dir1
  ```

  4. 将文件`file1.txt`的属主和所属群组修改为与文件`file2.txt`相同：

  ```
  chown --reference=file2.txt file1.txt
  ```

  

- `chgrp 所属组 文件名`：修改文件或目录的所属组。

  

  `chgrp`命令的使用格式如下：

  ```
  chgrp [选项] 群组名 文件或目录
  ```

  其中，选项常用的有以下两个：

  - `-R` 递归地更改目录及其下所有文件的所属群组。
  - `--reference=RFILE` 将指定文件的所属群组复制到另一个文件或目录。

  `群组名`表示要将该文件或目录的所属群组修改为指定的群组名或群组ID。

  下面是几个使用`chgrp`命令的例子:

  1. 将`file.txt`的所属群组修改为`group1`：

  ```
  chgrp group1 file.txt
  ```

  2. 将目录`/home/dir1`及其下所有文件的所属群组修改为`group1`：

  ```
  chgrp -R group1 /home/dir1
  ```

  3. 将文件`file1.txt`的所属群组修改为与文件`file2.txt`相同：

  ```
  chgrp --reference=file2.txt file1.txt
  ```


###### 三.文件打包与文件解包

tar是Linux系统中使用最广泛的归档工具，它可以把多个文件和目录打包成一个文件，并且可以提供压缩和解压缩功能。

tar命令的基本语法如下：

```
tar [选项] [文件名/目录名]
```

常用选项：

- -c：新建一个 tar 包，将指定文件或目录打包成 tar 包
- -x：从 tar 包中抽取文件
- -v：显示进程
- -f：写入文件，即指定 tar 包文件
- -z：使用 gzip 压缩文件
- -j：使用 bzip2 压缩文件
- -p：保留源文件权限和属性
- -P：保留绝对路径信息

常用示例：

1. 压缩文件
将文件 a.txt 和 b.txt 打包并且压缩为 a.tar.gz 文件。

```
tar -zcvf a.tar.gz a.txt b.txt
```

2. 解压文件

  将文件 a.tar.gz 解压到当前目录下。

```
tar -zxvf a.tar.gz
```

​	3.解压到指定目录
​	将文件 a.tar.gz 解压到 /usr/local 目录下。

```
tar -zxvf a.tar.gz -C /usr/local
```



###### 四.计划任务的管理

在CentOS中，计划任务是一种非常重要的管理工具，它允许用户在特定的时间或事件上运行自动化的脚本或命令。以下是在CentOS中管理计划任务的方法：

1. 创建计划任务
使用crontab命令可以创建新的计划任务，其基本语法为：

```shell
crontab [-u user] -e

#如果命令找不到命令请使用下面命令安装
yum install crontab
```



其中，user参数指定要创建计划任务的用户；-e选项表示打开cron表编辑器。

2. 编辑计划任务
使用crontab编辑器，可以打开一个配置文件，用于创建和编辑计划任务。对于每个计划任务，需要指定以下内容：

- 分 - 0-59之间的值
- 时 - 0-23之间的值
- 天 - 1-31之间的值
- 月 - 1-12之间的值
- 星期 - 0-7之间的值，0和7代表星期日

计划任务的语法如下(<u>分时日月周</u>)：

```
*     *     *     *     *  command to be executed
-     -     -     -     -
|     |     |     |     |
|     |     |     |     +----- day of the week (0 - 6) (Sunday = 0)
|     |     |     +------- month (1 - 12)
|     |     +--------- day of the month (1 - 31)
|     +----------- hour (0 - 23)
+------------- min (0 - 59)
```

例如，以下是每天3点运行脚本 /usr/bin/myscript.sh的计划任务：

```shell
0 3 * * * /usr/bin/myscript.sh
#:wq保存退出
#后提示crontab: installing new crontab表示计划任务创建/更新成功
#如果出现其他提示则失败
```

3. 查看计划任务
可以使用crontab命令查看当前用户的计划任务列表，其基本语法为：

```
crontab -l
```

如果需要查看其他用户的计划任务，可以使用以下命令：

```
crontab -u user -l
```

4. 删除计划任务
可以使用crontab命令删除计划任务，其基本语法为：

```
crontab -r
```

如果需要删除其他用户的计划任务，可以使用以下命令：

```
crontab -u user -r

```

###### 五.文件的查找

`find` 是一个功能强大的命令，可用于在文件系统中查找文件和目录。

`find` 命令的一般格式如下：

```
find [path] [expression]
```

其中，`path` 表示要查找的路径或目录，`expression` 表示要对找到的文件进行匹配的表达式。

`expression` 中可以包含一些选项和测试，例如 `-name` 选项用来指定要匹配的文件名或模式，`-size` 选项用来指定文件的大小，`-mtime` 选项用来指定文件的修改时间等。多个选项和测试可以组合使用，用 `-a`（默认为 and）或 `-o`（or）进行逻辑操作。

1. 按名称查找文件

要按名称查找文件，请使用 `-name` 选项。例如，要在当前目录及其子目录中查找所有名为 `example.txt` 的文件，请运行以下命令：

```shell
find . -name example.txt
#如果不指定查找路径,默认从当前路径查找,
#所以在这里.可以省略
```

其中，“.” 表示当前目录。

2. 按类型查找文件

要按类型查找文件，请使用 `-type` 选项。例如，要在当前目录及其子目录中查找所有目录，请运行以下命令：

```shell
find . -type d
```

同样地，要在当前目录及其子目录中查找所有普通文件，请运行以下命令：

```shell
find . -type f
```

3. 按时间戳查找文件

要按时间戳查找文件，请使用 `-mtime` 选项。例如，要在当前目录及其子目录中查找所有在过去7天内修改过的文件，请运行以下命令：

```shell
find . -type f -mtime -7
```

其中，“-7” 表示过去7天。您还可以使用 `+` 来查找在指定天数之前修改的文件。例如，要在当前目录及其子目录中查找所有超过30天前修改过的文件，请运行以下命令：

```shell
find . -type f -mtime +30
```

4. 执行操作

`find` 命令还可以用 `-exec` 选项来执行操作。例如，要在当前目录及其子目录中查找所有名为 `*.txt` 的文件并将它们复制到指定的目录，请运行以下命令：

```shell
find . -name "*.txt" -exec cp {} /path/to/destination \;
```

其中，“{}” 将被替换为每个匹配文件的名称，并使用 `\;` 来表示完成命令。



这只是一些 `find` 命令的常见用法。它还有许多其他选项和参数:

- `-name pattern` 按照文件名匹配模式 `pattern` 进行匹配。
- `-iname pattern` 不区分大小写地按照文件名匹配模式 `pattern` 进行匹配。
- `-type type` 按照文件类型 `type` 进行匹配。常用的 `type` 参数有：`f`（文件）、`d`（目录）、`l`（符号链接）。
- `-size n[unit]` 按照文件或目录的大小进行匹配。`n` 表示文件或目录的大小，`unit` 表示单位。常用的单位有：`k`（千字节）、`M`（兆字节）、`G`（千兆字节）。默认单位字节
- `-maxdepth n` 限制搜索目录的深度为 `n`，即向下搜索至深度 `n` 的子目录。

测试：

- `-mtime n` 按照文件或目录的修改时间进行匹配。`n` 表示相对于当前时间过去的天数。例如：`-mtime -7` 表示最近七天内修改过的文件。
- `-mmin n` 按照文件或目录的修改时间进行匹配。`n` 表示相对于当前时间过去的分钟数。例如：`-mmin -60` 表示最近一小时内修改过的文件。
- `-atime n` 按照文件或目录的访问时间进行匹配。`n` 的格式与 `-mtime` 相同。
- `-amin n` 按照文件或目录的访问时间进行匹配。`n` 的格式与 `-mmin` 相同。
- `-perm mode` 按照文件或目录的权限进行匹配。`mode` 是一个八进制数，表示文件或目录的权限。例如：`-perm 644` 表示权限为 `644` 的文件。

除了上述选项和测试之外，`可以使用 `man find` 命令查看完整的 `find` 命令手册。

###### 六:文件编辑

简单操作:

在CentOS系统中，您可以使用几个命令将文件打印到终端或打印机上。以下是其中的一些命令：

1. cat命令：使用cat命令可以将文件的内容打印到终端上。例如，要打印一个名为`example.txt`的文件，请在终端中运行以下命令：

```
cat example.txt
```

这将在终端上打印文件的全部内容。

2.touch命令：touch命令用于创建一个新的空文件。要创建一个名为`example.txt`的新文件，请在终端中输入以下命令：

```
touch example.txt
```

3.重定向>`和`>>`，用于将命令输出写入文件。

`>`符号用于覆盖目标文件的内容，并将命令输出写入文件。例如，要将命令的输出内容写入一个名为`example.txt`的文件中，请使用以下命令：

```shell
#将命令执行结果覆盖写入字符串
command > example.txt
#也可以将自定以字符串覆盖写入文件
echo "字符串" > example.txt
```

如果`example.txt`文件已经存在，则该命令将覆盖文件中的内容。如果文件不存在，则将创建一个新文件并将命令输出写入其中。

`>>`符号用于在现有文件中追加命令输出，而不是覆盖整个文件。例如，要将命令的输出追加到`example.txt`文件中，请使用以下命令：

```shell
#将命令执行结果覆盖写入字符串
command >> example.txt
#也可以将自定以字符串追加写入文件
echo "字符串" >> example.txt
```

如果`example.txt`文件不存在，则会创建一个新文件并将命令输出写入其中。如果文件已经存在，则命令的输出将被追加到文件的末尾，而不是覆盖整个文件.

vi/vim:

vim相比vi多了语法高亮。它们是命令行界面下很强大的编辑器，可用于编辑各种文件类型（例如代码、配置文件等）。

以下是一些常见的vi/vim操作：

- 调用vi/vim编辑器：在终端中输入以下命令即可启动vi/vim：

  ```shell
  vi filename
  #vim filename
  ```

  1. 移动光标：

     - `gg` 移动到文件的第一行
     - `G` 移动到文件的最后一行
     - `Home` 移动到行首
     - `End` 移动到行尾
     - `:set` nu显示行号
     - `:数字n` 跳转到第n行
  2. 插入和编辑文本：

     - `i` 进入插入模式，在当前光标处插入文本
     - `a` 进入插入模式，在当前光标的下一个字符插入文本
     - `o` 进入插入模式，在当前行下面插入新的一行并开始编辑文本
     - `u` 撤销最近的操作
     - `Ctrl + r` 重做最近的操作
  3. 保存和退出：
  - `:w` 保存文件
     - `:q` 退出编辑器
     - `:wq` 保存文件并退出编辑器
     - `:q!` 放弃所有更改并强制退出
     - `!`强制操作
  4. 查找、替换和删除：

     - `/<search_term>` 在文本中查找指定的搜索项
     - `n` 在文本中跳转到下一个匹配项
     - `N` 在文本中跳转到上一个匹配项
     - `:s/<old>/<new>/g` 全局替换文本中所有的旧字符串为新字符串
     - `ndd` 剪切光标往下的n行,如果没有n则剪切当前行
     - `nyy` 复制光标向下的n行,如果没有n则复制当前行
     - `p` 粘贴复制或删除的内容

  

以上只是vi/vim编辑器的基本操作，它们还有许多高级功能可以自己搜索

###### 七:AWK 命令



grep命令是一个非常强大的文本搜索工具，可以在文件或标准输入中查找指定字符串模式，并将匹配文本行打印到标准输出。

grep命令的基本语法如下：

```
grep [OPTIONS] PATTERN [FILE...]
```

其中，OPTIONS表示命令选项，PATTERN表示要查找的字符串模式，FILE是可选参数，用于指定要搜索的文件名。

常见的选项包括：

- -i：忽略字符大小写。
- -v：显示不包含指定模式的行。
- -n：显示匹配行所在的行号。
- -r：递归搜索指定目录下的子目录。
- -w：匹配整个单词而非子字符串。
- -l：仅列出包含匹配模式的文件名。

例如，使用grep命令在文件test.txt中查找所有包含"apple"的行：

```
grep "apple" test.txt
```

使用-i选项可以忽略大小写，例如：

```
grep -i "APPLE" test.txt
```

使用-n选项可以在匹配行前面显示行号：

```
grep -n "apple" test.txt
```

使用-l选项会列出包含匹配模式的文件名：

```
grep -l "apple" *
```

这条命令会在当前目录中查找包含"apple"模式的文件，并列出它们的文件名。



`awk` 是一种用于文本处理的强大工具，其功能与 `sed` 和 <u>`grep`(搜索)</u> 类似。通常情况下，`awk` 可以帮助您从文件或输入中提取和格式化数据。以下是常用的 `awk` 用法：

1. 输出指定列的内容：
   
   ```
   awk '{print $1, $2}' file.txt
   ```
   这个命令会输出 `file.txt` 文件中每一行的第一列和第二列的内容。

2. 使用分隔符来输出指定列的内容：
   
   ```
   awk -F"," '{print $1, $3}' file.txt
   ```
   这个命令会输出以逗号(,)作为分隔符的 `file.txt` 文件中每一行的第一列和第三列的内容。

3. 使用条件来输出内容：

   ```
   awk '$3 > 20 {print $1, $2}' file.txt
   ```
   这个命令会输出 `file.txt` 文件中第三列大于 20 的行的第一列和第二列的内容。

4. 使用内置变量：

   ```
   awk '{print NR, $0}' file.txt
   ```
   这个命令会输出 `file.txt` 文件中每一行的行号和整行内容。
   
5. 使用函数：

   ```
   awk '{print length($0)}' file.txt
   ```
   这个命令会输出 `file.txt` 文件中每一行的字符数。

6. 在匹配模式后执行操作：

   ```
   awk '/pattern/ {print $0}' file.txt
   ```
   这个命令会输出 `file.txt` 文件中包含 `pattern` 的行。

7. 将多个文件合并并进行处理：

   ```
   awk '{print $0}' file1.txt file2.txt
   ```
   这个命令会将 `file1.txt` 和 `file2.txt` 中的所有行输出。

8. 使用正则表达式进行模式匹配：

   ```
   awk '/pattern/ {print $0}' file.txt
   ```
   这个命令会输出 `file.txt` 文件中包含 `pattern` 的行。

9. 计算行数和列数：

   ```
   awk 'END{print NR, NF}' file.txt
   ```
   这个命令会输出 `file.txt` 文件中的总行数和总列数。

10. 对数据进行求和和统计：

    ```
    awk '{sum+=$1} END {print sum}' file.txt
    ```
    这个命令会输出 `file.txt` 文件中第一列的值的总和。

11. 处理文件中重复的行：

    ```
    awk '!seen[$0]++' file.txt
    ```
    这个命令会输出 `file.txt` 文件中不重复的行。

12. 更改字段分隔符：

    ```
    awk -F':' '{print $1, $2}' file.txt
    ```

    这个命令会输出 `file.txt` 文件中每一行以冒号 (`:`) 作为分隔符的第一列和第二列的内容。

13. 使用 `awk` 内置变量操作数据：

     ```
     awk '{sum+=$1} END {print "Average = ",sum/NR}' file.txt
     ```

     这个命令会输出 `file.txt` 文件第一列数值的平均值。

14. 处理多个文件并输出含有某个关键字的行：

     ```
     awk '/keyword/ {print FILENAME ":", $0}' file1.txt file2.txt
     ```

    这个命令会输出 `file1.txt` 和 `file2.txt` 中含有关键字 `keyword` 的行，并在每行前面加上该行所在的文件名。

    

以上是一些常见的 `awk` 用法，更多用法可以自行search

###### 八:SED 命令

`sed`（Stream Editor）是一种在命令行下使用的文本编辑器，它可以对文本流（文件或管道中的数据）进行操作，并支持搜索、替换、删除、插入等功能。以下是一些常用的sed用法：

1. 替换文本：`s/old/new/g`，将所有出现的`old`替换为`new`。

例如：将文件file.txt中所有的"apple"替换成"orange"，命令如下：
```
sed 's/apple/orange/g' file.txt
```

2. 在指定行前或后添加新行：`a\text` 或 `i\text`，其中`a`表示在指定行后添加，`i`表示在指定行前添加。

例如：在文件file.txt的第3行后加入一行"This is a new line"，命令如下：
```
sed '3a\This is a new line' file.txt
```

3. 删除指定行：`d`，删除指定行。

例如：删除文件file.txt中第5行，命令如下：
```
sed '5d' file.txt
```

4. 输出指定行：`p`，输出符合条件的行。

例如：输出文件file.txt中第3行，命令如下：
```
sed -n '3p' file.txt
```

5. 组合多条命令：使用分号将多个命令连接起来。

例如：将文件file.txt中所有的"apple"替换成"orange"并将结果覆盖到源文件，同时将修改后的文件内容输出到终端，命令如下：
```
sed -i 's/apple/orange/g; p' file.txt
```

6. 修改文件内容并保存到新文件：使用重定向符号将输出结果写入到新文件中。

例如：将文件file.txt中所有的"apple"替换成"orange"并将修改后的内容保存到新文件newfile.txt中，命令如下：
```
sed 's/apple/orange/g' file.txt > newfile.txt
```

7. 从文件中读取sed脚本：使用`-f`参数指定脚本文件名。

例如：将sed脚本内容保存到文件myscript.sed中，然后使用该脚本修改file.txt文件，命令如下：
```
sed -f myscript.sed file.txt
```

8. 正则表达式的使用：`sed`支持各种正则表达式的使用，可以使用正则表达式实现更加灵活的文本匹配和替换操作。

例如：在文件file.txt中查找所有以数字开头的行，并将这些行的开头部分替换成"Number: "，命令如下：
```
sed '/^[0-9]/s/^/Number: /' file.txt
```

9. 处理多个文件：使用通配符或多次执行命令的方式从多个文件中处理。

例如：将目录中所有以".txt"结尾的文件中的"apple"替换成"orange"，命令如下：
```
sed -i 's/apple/orange/g' *.txt
```

10. 反向引用：在替换操作中使用圆括号将匹配的文本分组，然后使用反向引用（`\1`、`\2`等）来引用这些分组。

例如：将文件file.txt中所有形如"First Name: Last Name"的行中的"Last Name"提取出来，转为"Last Name: First Name"的格式。命令如下：
```
sed 's/\(.*\): \(.*\)/\2: \1/' file.txt
```

11. 修改指定行的内容：在指定行进行修改。

例如：将文件file.txt中第3行的内容修改为"This is the new content of line 3."，命令如下：
```
sed '3c\This is the new content of line 3.' file.txt
```

12. 替换指定区间的内容：使用行号或模式匹配来指定替换的区间。

例如：将文件file.txt中从第3行到第6行的内容都替换为"New content"，命令如下：
```
sed '3,6s/.*/New content/' file.txt
```

13. 高级模式匹配：使用正则表达式中的限定符、反向引用等高级功能实现更复杂的模式匹配。

例如：将文件file.txt中所有连续出现的数字串合并为一个整数，命令如下：
```
sed -E 's/([0-9]+)[ \t]*([0-9]+)/\1\2/g' file.txt
```

14. 使用变量和大括号：使用大括号将变量和字符串拼接在一起，该功能需要使用`-E`参数打开扩展的正则表达式功能。

例如：将文件file.txt中所有"apple"替换为变量$fruit的值，并用大括号将变量和字符串拼接为一个整体，命令如下：
```
fruit=orange
sed -E "s/{apple}/$fruit/g" file.txt
```



以上是sed的一些常用用法，更多用法请自行search

###### 九.管道与数据流重定向

Linux管道和数据流重定向是Linux系统中常用的工具，可以将多个命令的输出串联起来，也可以将命令的输入或输出重定向到特定的文件或设备。

管道操作符 "| "用于连接多个命令，将前一个命令的输出作为下一个命令的输入。例如，下面的命令将列出当前目录下的所有文件，并使用grep命令过滤以".txt"结尾的文件：

```
ls | grep ".txt"
```

数据流重定向有三种形式: ">" , "<", ">>"。

"> "操作符用于将命令的输出重定向到指定的文件中，如果该文件不存在，则会创建该文件。

例如，下面的命令将把当前目录下的所有.txt文件的名称写入到filelist.txt文件中：

```
ls *.txt > filelist.txt
```

"<"操作符将文件中的内容作为命令的输入。例如，下面的命令将使用sort命令对filelist.txt文件中的行进行排序：

```
sort < filelist.txt
```

">>"操作符用于将命令的输出追加到指定的文件中，如果该文件不存在，则会创建该文件。

例如下面的命令将列出当前目录下的所有文件并将结果追加到filelist.txt中：

```
ls >> filelist.txt
```

###### 十.Linux常用命令

```powershell

1. ls - 列出当前目录下的文件和子目录。
2. cd - 进入指定目录。
3. pwd - 显示当前目录路径。
4. mkdir - 创建新目录。
5. touch - 创建新文件或更改现有文件的时间戳。
6. cat - 查看文件内容。
7. rm - 删除文件或目录。
8. mv - 移动或重命名文件或目录。
9. cp - 复制文件或目录。
10. ps - 显示进程状态。
11. top - 实时显示进程资源占用情况。
12. systemctl - 控制 systemd 系统和服务管理器。
13. yum - 安装、更新和删除软件包。
14. nano - 编辑文本文件。
15. vi - 强大的文本编辑器。
16. grep - 在文件中搜索指定的文本模式。
17. find - 在目录和子目录中查找文件和目录。
18. tar - 压缩和解压文件和目录。
19. gzip - 压缩文件。
20. gunzip - 解压缩文件。
21. chown - 更改文件或目录的所有者。
22. chmod - 更改文件或目录的访问权限。
23. ssh - 远程登录到另一台计算机。
24. scp - 安全地复制文件到远程计算机。
25. ping - 测试网络连接。
26. ifconfig - 显示网络接口配置信息。
27. netstat - 显示网络连接和路由表信息。
28. hostname - 显示或设置主机名。
29. whoami - 显示当前用户的用户名。
30. su - 用于切换到不同的用户身份。
31. sudo - 以超级用户权限运行命令。
32. df - 显示磁盘空间使用情况。
33. du - 显示目录和文件的磁盘使用情况。
34. uname - 显示系统和内核信息。
35. date - 显示当前日期和时间。
36. cal - 显示日历。
37. tee - 将输出复制到文件和终端。
38. awk - 处理文本文件中的数据。
39. sed - 文本编辑器，用于查找、替换、删除和插入文本。
40. curl - 从 Web 服务器下载文件。
41. wget - 从 Web 服务器下载文件。
42. tar -xzvf filename.tar.gz 解压 .tar.gz 文件。
43. history - 显示已执行的命令。
44. tail -n - 显示文本文件的结尾部分。
45. head -n - 显示文本文件的开头部分。
46. diff - 比较两个文件内容的不同之处。
47. ssh-keygen - 生成 SSH 密钥对。
48. scp -r - 安全地复制目录到远程计算机。
49. ps aux | grep processname - 查找正在运行的进程。
50. systemctl start servicename - 启动 systemd 服务。
51. systemctl stop servicename - 停止 systemd 服务。
52. systemctl restart servicename - 重新启动 systemd 服务。
53. systemctl enable servicename - 开机启动 systemd 服务。
54. systemctl disable servicename - 关闭开机启动 systemd 服务。
55. tar -czvf filename.tar.gz directoryname/ - 压缩目录到 .tar.gz 文件。
56. rm -r directoryname/ - 删除目录和其中的文件。
57. who - 显示当前登录用户信息。
58. w - 显示当前登录用户信息。
59. last -n - 显示最近登录的用户。
60. man commandname - 显示命令的手册页。
61. info commandname - 显示命令的信息页。
62. locate filename - 查找文件。
63. updatedb - 更新 locate 数据库。
64. lsof -i :portnumber - 显示正在使用指定端口号的进程。
65. uname -a - 显示系统和内核信息。
66. uptime - 显示系统运行时间、负载平均值和用户数。
67. free -m - 显示系统内存使用情况。
68. df -h - 显示磁盘空间使用情况(以 GB 为单位)。
69. du -h - 显示目录或文件的磁盘使用情况(以 GB 为单位)。
70. tar -xvf filename.tar - 解压 .tar 文件。
71. tar -cvf filename.tar directoryname/ - 压缩目录到 .tar 文件。
72. tar -xzvf filename.tgz - 解压 .tgz 文件。
73. tar -czvf filename.tgz directoryname/ - 压缩目录到 .tgz 文件。
74. tar -cjvf filename.tar.bz2 directoryname/ - 压缩目录到 .tar.bz2 文件。
75. tar -xjvf filename.tar.bz2 - 解压 .tar.bz2 文件。
76. yum search keyword - 在软件包库中查找关键字。
77. yum install packagename - 安装软件包。
78. yum remove packagename - 删除软件包。
79. yum update - 更新所有已安装的软件包。
80. iptables -L - 显示防火墙规则列表。
81. ifconfig eth0 up - 启用网络接口 eth0。
82. ifconfig eth0 down - 禁用网络接口 eth0。
83. ifup eth0 - 启用网络接口 eth0。
84. ifdown eth0 - 禁用网络接口 eth0。
85. ping -c 3 hostname - 测试主机名的连通性。
86. ping -c 3 IPaddress - 测试 IP 地址的连通性。
87. traceroute hostname - 显示从本地到目标主机的网络路径。
88. traceroute IPaddress - 显示从本地到目标 IP 地址的网络路径。
89. ssh username@hostname - 远程登录到另一台计算机。
90. scp localfile remoteuser@remotehost:remotefile - 将本地文件复制到远程计算机。
91. scp remoteuser@remotehost:remotefile localfile - 将远程文件复制到本地计算机。
92. chmod +x filename - 赋予 filename 可执行权限。
93. chown username filename - 将文件的所有者更改为 username。
94. chgrp groupname filename - 更改文件所属组为 groupname。
95. ps -ef | grep processname - 查找正在运行的进程。
96. top -n 1 -b > topoutput.txt - 将 top 命令输出保存到文件。
97. netstat -a - 显示所有网络连接。
98. netstat -an - 显示所有网络连接及其端口号。
99. netstat -rn - 显示路由表信息。
100. grep -r pattern directoryname/ - 在目录及其子目录中查找文本模式。
```



# 模块B:数据库系统

个人需要知道和理解：

```
 Linux 操作系统下 MySQL 服务相关的操作
 SQL 标准化查询语言
 数据库的增删改
 数据表的增删改查
```



个人应能够：

```
 掌握 Linux 下 MySQL 的安装、配置与管理
 使用 SQL 完成数据库的增删改操作
 使用 SQL 完成数据表的增删改查与导入导出操作
 使用 SQL
```



## 主要知识和技能点:

```
搭建配置 MySQL 服务、对库进行配置 操作创建、查询、删除、对表进行操作创建、导数据、查询、删
除等
```

###### 一.Centos使用rpm离线安装mysql5

1.卸载centos自带的mariadb,否则会因为版本兼容问题导致无法安装

```shell
#查找 MariaDB 是否安装
rpm -qa | grep mariadb
#如果查找到MariaDB,通过下面命令卸载,注意软件包版本要为查找出来的版本
#-e选项表示卸载
#--nodeps表示不检查依赖强制卸载
rpm -e --nodeps mariadb-libs-xxxxxxxxxxx
#执行命令没有打印错误信息即为卸载成功
```

2.解压mysql包到当前目录

```
cd /opt/software
tar -zxvf mysql_5.7.36-1.tar.gz
```

3.使用rpm安装mysql服务(**必须按顺序安装**)

```shell
#rpm是mysql的包管理器,用于安装/卸载软件包
#-i选项表示安装
#-v选项打印安装信息
#-h选择打印进度条
rpm -ivh mysql-community-common-5.7.36-1.el7.x86_64.rpm 

rpm -ivh mysql-community-libs-5.7.36-1.el7.x86_64.rpm 

rpm -ivh mysql-community-client-5.7.36-1.el7.x86_64.rpm 

rpm -ivh mysql-community-server-5.7.36-1.el7.x86_64.rpm 

#不报错则安装成功

```

4.修改配置文件

```shell
vi /etc/my.cnf
#将下面内容追加到配置文件中
default-storage-engine=innodb
innodb_file_per_table
collation-server=utf8_general_ci
init-connect='SET NAMES utf8'
character-set-server=utf8

```

5.启动mysqld的服务并查看状态

```shell
#启动服务
systemctl start mysqld
#查看服务状态
systemctl status mysqld
#active (running)启动成功




#如果启动服务报错,修改完配置文件后,先删除mysql的数据文件,在重启mysqld服务
#默认数据文件存储位置为/var/lib/mysql
rm -vrf /var/lib/mysql
```



6.修改默认密码

```shell
#打印系统生成的临时密码
cat /var/log/mysqld.log|grep password

#使用临时密码登录
mysql -uroot -p

#设置mysql密码安全策略,否则无法设置简单密码
set global validate_password_policy=low;#设置安全策略为低
set global validate_password_length = 0;#设置最短密码长度为0

 

#设置本地root用户登录密码`your_password`为实际密码
ALTER USER 'root'@'localhost' IDENTIFIED BY 'your_password';

```

7.配置允许远程访问

```shell
#`your_password`为实际密码
grant all privileges on *.* to root@'%' identified by 'your_password' with grant option;

#测试是否成功,`ip`是连接主机的实际ip
mysql -uroot -p -h ip
#如果配置无误,但无法连接请检查防火墙,刷新表,或者设置root@%的密码
```

###### 二.常用SQL

SQL 是结构化查询语言（Structured Query Language）的简称，是一种用于管理关系数据库管理系统（RDBMS）的标准语言。以下是一些常用的 SQL 查询(**SQL不区分大小写,可根据习惯书写,表名字段名除外**)

###### 1.CREATE

用于创建数据库对象，包括库、表、视图、存储过程等。以下是一些常见的CREATE语句用法：

- 创建数据库：使用CREATE DATABASE语句。

  ```mysql
  CREATE DATABASE db_name;
  #进入数据库 
  use database_name;
  ```

- 创建表：使用CREATE TABLE语句(**请自行搜索并掌握mysql的数据类型**)。

  ```mysql
  CREATE TABLE table_name (
      column1 datatype,
      column2 datatype,
      column3 datatype,
      ...
  );
  
  #查看表结构
  desc table_name;
  ```

- 创建视图：使用CREATE VIEW语句。

  ```mysql
  CREATE VIEW view_name AS
  SELECT column1, column2, ...
  FROM table_name
  WHERE condition;
  ```

- 创建存储过程：使用CREATE PROCEDURE语句。

  ```mysql
  CREATE PROCEDURE procedure_name
  AS
  BEGIN
      -- 执行过程逻辑
  END;
  ```

- 创建索引：使用CREATE INDEX语句。

  ```mysql
  CREATE INDEX index_name ON table_name (column1, column2, ...);
  ```

###### 2.SELECT

 是 SQL 中最常用的关键字之一，用于从数据库中检索数据。

1. 基本用法：使用 `SELECT` 连接列名以检索数据库表的数据。
   
```mysql
   SELECT column1, column2, ... FROM table_name;
```

   

2. 使用通配符（`*`）：使用通配符可以方便地选择所有列。
   
```mysql
   SELECT * FROM table_name;
```

   

3. 带有别名的使用：为列指定别名可以使查询结果更易读和理解。
   
```mysql
   SELECT column1 AS col1, column2 AS col2 FROM table_name;
```

   

4. 使用 `DISTINCT` 关键字：使用 `DISTINCT` 对查询结果去重。
   
```mysql
   
   SELECT DISTINCT column1 FROM table_name;
```

   

5. 使用 WHERE 子句：使用 `WHERE` 子句过滤行。
   
```mysql
   SELECT column1, column2 FROM table_name WHERE condition;
```

   

6. 使用 ORDER BY 子句：使用 `ORDER BY` 根据指定列对结果进行排序。
   
```mysql
   #orderby 默认为asc 升序,可以指定降序desc
   SELECT column1, column2 FROM table_name ORDER BY column1 ASC;
```

   

7. 使用 GROUP BY 子句：使用 `GROUP BY` 将结果按指定列分组。
   
```mysql
   
   SELECT column1, COUNT(*) FROM table_name GROUP BY column1;
```

   

8. 使用 HAVING 子句：使用 `HAVING` 子句过滤组()。
   
```mysql
   #分组后筛选只能用having
   SELECT column1, COUNT(*) FROM table_name GROUP BY column1 HAVING COUNT(*) > 1;
```

   

9. 使用 LIMIT 子句：使用 `LIMIT` 限制结果集的大小。
   
```mysql
   #limit n显示前n行
   #limit n m从第n行往下显示m行
   SELECT column1, column2 FROM table_name LIMIT 5;
```

   

10. 使用 OFFSET 子句：使用 `OFFSET` 跳过前面的几行。
    
    ```mysql
    SELECT column1, column2 FROM table_name LIMIT 5 OFFSET 10;
    ```
    
11. 使用JOIN子句：使用JOIN子句将多个表格联接起来，生成新的虚拟表格。
    
    ```mysql
    SELECT column1, column2 FROM table1 INNER JOIN table2 ON table1.id=table2.id;
    ```
    
    
    
12. 使用UNION子句：使用UNION将两个或更多SELECT语句的结果集合并到一个数据集中。
    
```mysql
    SELECT column1, column2 FROM table1 UNION SELECT column1, column2 FROM table2;
```


​    
13. 使用INTERSECT子句：使用INTERSECT返回两个或更多SELECT语句的结果集的交集。
    
```mysql
    SELECT column1, column2 FROM table1 INTERSECT SELECT column1, column2 FROM table2;
```


​    
14. 使用EXCEPT子句：使用EXCEPT从第一个查询结果集中返回不在第二个查询结果集中的结果。
    
```mysql
    SELECT column1, column2 FROM table1 EXCEPT SELECT column1, column2 FROM table2;
```


​    
15. 使用BETWEEN和IN子句：使用BETWEEN和IN子句搜索一定范围内或一组给定值的数据。
    
    ```mysql
    SELECT column1, column2 FROM table_name WHERE column1 BETWEEN value1 AND value2;`
          `SELECT column1, column2 FROM table_name WHERE column1 IN (value1, value2, ...);
         
    ```

16. 使用LIKE子句：使用LIKE子句可以根据模糊匹配的方式检索数据。
    
```mysql
    SELECT column1, column2 FROM table_name WHERE column1 LIKE 'value%';
```


​    
17. 使用CASE语句：使用CASE语句可以根据不同的条件选择不同的结果。
    
```mysql
    SELECT column1, column2, CASE WHEN column3='value1' THEN 'result1' ELSE 'result2' END AS result FROM table_name;
```


​    
18. 使用聚合函数：使用聚合函数可以计算数据表中某列的总和、平均值、最大、最小等统计信息。
    
```mysql
    SELECT COUNT(column1), SUM(column2), AVG(column3), MIN(column4), MAX(column5) FROM table_name;
```


​    
19. 使用子查询：使用子查询可以在 SELECT、INSERT、UPDATE 或 DELETE 语句内部嵌入内部查询，在内部查询返回结果后，将其传递给主查询。
    
```mysql
    SELECT column1, column2 FROM table1 WHERE column1 IN (SELECT column1 FROM table2);
```


​    
20. 使用窗口函数：窗口函数是一种在结果集上执行聚合计算、排序、排名和分配行编号的方法。
    
    ```mysql
    SELECT column1, column2, SUM(column3) OVER (PARTITION BY column4 ORDER BY column5 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS result FROM table_name;
    ```
    
21. 使用WITH子句：WITH 子句可以定义一个SQL查询中可被多次引用的临时结果集，有助于简化复杂查询。 例

    ```mysql
    WITH temp_table AS (SELECT column1, column2 FROM table_name WHERE column3='value1') SELECT * FROM temp_table WHERE column2='value2';
    ```

    以上是 `SELECT` 的基本用法，更多方法请自行search

###### 3.INSERT INTO

 是 SQL 中用来向表中插入新数据的语句，其基本语法如下：

```mysql
INSERT INTO 表名 (列名1, 列名2, 列名3, ...) VALUES (值1, 值2, 值3, ...);
```

其中：
- `表名`：需要插入数据的表的名称。
- `(列名1, 列名2, 列名3, ...)` ：需要插入数据的列的列表。如果省略列名，则默认插入表中所有列。
- `VALUES` ：需要插入的数据的值列表，顺序与列名列表对应。

例如，以下语句向 `users` 表插入一行数据：

```mysql
INSERT INTO users (name, age, gender) VALUES ('Tom', 18, 'Male');
```

这个语句将在 `users` 表中，插入一行姓名为 "Tom"，年龄为 18，性别为 "Male" 的新记录。

插入多行数据

可以使用多个值列表向表中插入多行数据。例如：

```mysql
INSERT INTO users (name, age, gender) VALUES ('Tom', 18, 'Male'), ('Lucy', 20, 'Female'), ('Jack', 22, 'Male');
```

这个语句将在 `users` 表中依次插入名为 Tom、Lucy 和 Jack 的三行记录。

<u>从其他表中选择的数据</u>

使用 `SELECT` 语句从其他表中选择数据作为新数据的值。例如：

```mysql
INSERT INTO users (name, age, gender) SELECT name, age, gender FROM temp_users WHERE age > 20;
```

此语句将从 `temp_users` 表中选择年龄大于 20 的记录，并将它们插入到 `users` 表中。

插入默认值

默认情况下，如果省略了列名和值列表，则 INSERT INTO 语句将使用表中所有列和相应的默认值（如果有的话）。例如：

```mysql
INSERT INTO users DEFAULT VALUES;
```

此语句将向 `users` 表中插入一条带有所有默认值的新记录。

插入子查询结果

可以使用子查询语句作为新数据的值。例如：

```mysql
INSERT INTO users (name, age, gender) SELECT name, age, 'Male' FROM temp_users WHERE gender = 'Male';
```

此语句将从 `temp_users` 表中选择性别为男性的记录，并将其插入到 `users` 表中，其中插入的新记录的性别是固定的 "Male"。

插入部分列

INSERT INTO 语句还可以只插入表中某些列的值。例如：

```mysql
INSERT INTO users (name, age) VALUES ('Tom', 18);
```

此语句将向 `users` 表中插入一条姓名为 "Tom"，年龄为 18 的新记录，而性别这一列的值将默认为 NULL。



除了以上基本语法，其他方法请自行search

###### 4.UPDATE

 命令用于更新关系型数据库中现有的记录数据。以下是 SQL Update 命令的几种用法：

1. 更新表中<u>所有记录</u>的一个或多个字段：
   ```mysql
   UPDATE table_name SET column1 = value1, column2 = value2;
   ```

2. 更新表中特定条件的记录的一个或多个字段：
   ```mysql
   UPDATE table_name SET column1 = value1, column2 = value2, ... WHERE condition;
   ```

3. 更新表中特定条件的记录，将一个字段的值增加一个特定数值：
   ```mysql
   UPDATE table_name SET column1 = column1 + value WHERE condition;
   ```

4. 更新表中特定条件的记录，将一个字段的值设置为另一个字段的值：
   ```mysql
   UPDATE table_name SET column1 = column2 WHERE condition;
   ```

5. 更新来自 join 表格的查询结果中的记录：
   ```mysql
   UPDATE table_name1 SET column1 = value1 FROM table_name1 INNER JOIN table_name2 ON condition;
   ```

###### 5.DELETE

 命令用于从关系型数据库中删除记录数据。以下是 SQL Delete 命令的几种用法：

1. 删除表中所有记录,保留表结构：
   ```mysql
   DELETE FROM table_name;
   ```

2. 删除表中特定条件的记录：
   ```mysql
   DELETE FROM table_name WHERE condition;
   ```

3. 删除来自 join 表格的查询结果中的记录：
   ```mysql
   DELETE table_name1 FROM table_name1 INNER JOIN table_name2 ON condition;
   ```

注意：在执行 SQL Delete 命令之前，请谨慎考虑操作的影响，确保正确使用 WHERE 子句以避免无意中删除错误的记录。 

4. 删除表中所有记录，然后重置自动增长的计数器：

   ```mysql
   TRUNCATE TABLE table_name;
   ```

6. 删除表格中特定条件的记录，并将它们插入另一个表格：
   ```mysql
   DELETE FROM table1 WHERE condition OUTPUT DELETED.* INTO table2;
   ```

8. 删除表中重复记录：
   ```sql
   DELETE t1 FROM table_name t1, table_name t2 WHERE t1.id < t2.id AND t1.column_name = t2.column_name;
   ```
   



######  6.DROP

语句用于删除数据库、表、视图等对象。它的一般语法如下：

- 删除整个数据库: `DROP DATABASE database_name;`
- 删除指定表格: `DROP TABLE table_name;`
- 删除视图: `DROP VIEW view_name;`
- 删除存储过程: `DROP PROCEDURE procedure_name;`
- 删除函数: `DROP FUNCTION function_name;`
- 删除触发器: `DROP TRIGGER trigger_name;`

###### 三.MySql数据导出导入

1.数据导出

使用以下命令导出整个数据库：

```bash
mysqldump -u 用户名 -p 数据库名 > 导出的文件名.sql
```

例如，如果要导出名为 `mydatabase` 的 MySQL 数据库，可以运行：

```bash
mysqldump -u root -p mydatabase > mydatabase.sql
```

该命令将把 `mydatabase` 数据库的所有表和数据导出到名为 `mydatabase.sql` 的文件中。

如果只需要导出某个表，可以使用以下命令：

```bash
mysqldump -u 用户名 -p 数据库名 表名 > 导出的文件名.sql
```

例如，如果要导出名为 `mytable` 的表，可以运行：

```bash
mysqldump -u root -p mydatabase mytable > mytable.sql
```

也可以在登录mysql cli后使用`SELECT INTO OUTFILE` 或者 `SELECT...INTO DUMPFILE`导出结果集或者二进制数据,具体可以自己search

2.数据导入

使用以下命令将导出的数据库或表导入 MySQL 数据库：

```bash
mysql -u 用户名 -p 数据库名 < 导入的文件名.sql
```

例如，如果要将名为 `mydatabase.sql` 的文件导入名为 `mydatabase` 的 MySQL 数据库，可以运行：

```bash
mysql -u root -p mydatabase < mydatabase.sql
```

如果导出的是某个表，可以使用以下命令将其导入到 MySQL 数据库：

```bash
mysql -u 用户名 -p 数据库名 < 导入的文件名.sql 表名
```

例如，如果要将名为 `mytable.sql` 的文件导入名为 `mydatabase` 的 MySQL 数据库中名为 `mytable` 的表，可以运行：

```bash
mysql -u root -p mydatabase < mytable.sql mytable
```

也可以登录mysql cli后使用source 命令导入数据.

# 模块C:Hadoop集群部署

个人需要知道和理解：

```
 Zookeeper 工作机制与配置参数
 SSH 免密码登录原理
 时间同步服务 NTP 原理
 Java 开发与运行环境
 Hadoop 完全分布式集群与相关服务
 Hadoop 配置文件的参数与意义
```

个人应能够：

```
 掌握 JDK 的安装
 掌握 SSH 免密码登录的配置
 掌握 NTP 服务的部署
 掌握 Zookeeper 的部署
 掌握 Hadoop 完全分布式集群的部署与使用
 掌握动态增加、删除 Hadoop 集群节点
 掌握 Hadoop 集群的调优点与故障解决
```

## 主要知识和技能点:

```
配置基础环境、Hadoop 完全分布式集群搭建 、 集群动态删除 DataNode 节点、集群调优与运维等
```

###### 一.基础环境搭建

###### 1.配置网络

**比赛时根据按照比赛要求决定是否配置**

```shell
#配置三台主机网络,根据实际情况修改
#vim和vi一样
#ens33是网卡名,不同主机可能不同
vim /etc/sysconfig/network-scripts/ifcfg-ens33

#修改或添加配置项
BOOTPROTO="static" #静态ip
ONBOOT="yes" #网卡自启动
IPADDR=192.168.85.11 #ip
NETMAST=255.255.255.0 #子网掩码
GATEWAY=192.168.85.2 #网络
DNS=114.114.114.114 #域名
#重启网卡服务使配置生效
systemctl restart network
```

###### 2.配置三台主机名和hosts文件

**比赛时根据按照比赛要求决定是否配置**

三台机器分别为:

- master 192.168.85.11
- slave1 192.168.85.12
- slave2 192.168.85.13

修改主机名

```shell
#修改三台主机名
hostname 主机名 #临时修改主机名,重启失效
#hostnamectl set-hostname 主机名 #永久修改主机名

bash #刷新使其生效

#在其他两台机器执行相同命令
```

修改hosts文件

```mysql
#修改hosts文件(即主机名与ip的映射关系)
vim /etc/hosts
#在末行追加下面内容       ip和主机名根据实际情况修改
192.168.85.11 master
192.168.85.12 slave1
192.168.85.13 slave2

#在其他两台执行相同命令
#或者使用下面scp命令分发到其他机器         ip为其他机器的实际ip,
scp /etc/hosts slave1:/etc
scp /etc/hosts slave2:/etc

#最后可以使用ping检测是否配置成功
```

###### 3.配置ssh免密

生成本机密钥并分发至集群

```shell
[root@master /]# ssh-keygen -t rsa
#一直回车
#将密钥分发至所有主机
[root@master /]# ssh-copy-id master
[root@master /]# ssh-copy-id slave1
[root@master /]# ssh-copy-id slave2
```

分别在slave1、slave2上也执行上述命令

测试免密是否配置成功

```shell
[root@master /]# ssh slave1
Last login: Tue Mar 28 20:28:14 2023 from 192.168.10.1
[root@slave1 ~]# 
```

###### 4.关闭防火墙

防火墙默认会禁用许多服务,比如zookeeper,所以要关闭它

```shell
#关闭防火墙
systemctl stop firewalld
#关闭防火墙自启动
systemctl disable firewalld
```

###### 5.配置ntp时间同步

时间不同步会导致hadoop启动失败,hive启动失败等,所以需要配置时间同步或者三台机器使用

要进行NTP时间同步配置，您可以按照以下步骤进行：

1. 安装NTP软件

在Linux系统上，您可以使用以下命令(<u>在线</u>)安装NTP软件：

```
sudo yum -y install ntp
```

2. master:配置NTP服务器

您可以在 `/etc/ntp.conf` 文件中配置NTP服务器。找到 `server` 关键字，设置同步服务器为master

例如：

```shell
#注释掉自带的

#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
#添加下面内容
server 127.127.1.0
fudge 127.127.1.0 stratum 10
```

3. slave1:配置ntp客户端

```shell
#注释掉自带的
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
#添加服务段ip
server 192.168.85.11

#scp分发到第三台机器
```



4.三台机器启动NTP服务并查看状态

执行以下命令来启动NTP服务：

```shell
systemctl start ntpd
systemctl status ntpd
```

5.验证时间同步

使用以下命令来检查时间同步是否正常运行：

```shell
#查看时间同步服务器
ntpq -p
#手动时间同步(默认64秒)
ntpdate -u 192.168.85.11
#检查三台机器时间
date

#设置ntp自启动
systemctl enable ntpd
```



或者三台机器同时使用<u>date -s "2023-05-01 13:38:00"</u>设置日期时间

###### 二.安装jdk和hadoop完全分布

###### 1.安装jdk并配置相关环境变量

进入/opt/software目录下,解压jdk和hadoop

```shell
[root@master software]# tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
[root@master software]# tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
```

解压完成后，进入/opt/module目录下，修改目录名（便于配置路径）

```shell
[root@master module]# mv jdk1.8.0_212/ jdk
[root@master module]# mv hadoop-3.1.3/ hadoop
```

配置环境变量,进入/etc/profile文件

```shell
[root@master module]# vim /etc/profile
```

在文件底部添加如下

```shell
export JAVA_HOME=/opt/module/jdk
export HADOOP_HOME=/opt/module/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin


```

保存后，需要执行source命令使环境变量生效

```shell
[root@master module]# source /etc/profile 
```

测试环境变量是否配置成功

```shell
[root@master module]# java -version
java version "1.8.0_212"
Java(TM) SE Runtime Environment (build 1.8.0_212-b10)
Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)
[root@master module]# hadoop version
Hadoop 3.1.3
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579
Compiled by ztang on 2019-09-12T02:47Z
Compiled with protoc 2.5.0
From source with checksum ec785077c385118ac91aadde5ec9799
This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar
[root@master module]# 
```

###### 2.修改hadoop配置文件

进入/opt/module/hadoop/etc/hadoop目录

修改hadoop-env.sh,末行追加

```sh
export JAVA_HOME=/opt/module/jdk
#hadoop3.0以上必须配置
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

修改core-site.xml

```xml
<configuration>              <!-- 配置文件标签 -->
<property>                   <!-- 参数配置标签 -->
	<name>fs.defaultFS</name>        <!-- Hadoop 文件系统的默认 URL 地址 -->
    <value>hdfs://master:9000</value>  <!-- Hadoop 文件系统地址，这里指定为名为 "master" 的 DataNode 节点，端口号为 9000 -->
</property>
<property>
    <name>hadoop.tmp.dir</name>     <!-- 指定 Hadoop 临时文件存储路径 -->
    <value>/opt/module/hadoop/tmp</value>  <!-- 临时文件存储路径 -->
</property>
</configuration>            <!-- 配置文件标签结束 -->
```

修改hdfs-site.xml

```xml
<configuration>              <!-- 配置文件标签 -->
<property>                   <!-- 参数配置标签 -->
	<name>dfs.replication</name>        <!-- HDFS 数据块的副本数 -->
    <value>3</value>                    <!-- 每个数据块默认的副本数目 -->
</property>
<property>
	<name>dfs.namenode.secondary.http-address</name>   <!-- SecondaryNameNode HTTP 访问地址 -->
    <value>slave1:50090</value>                      <!-- SecondaryNameNode 的通信接口，指定为名为 "slave1" 的节点，端口号为 50090 -->
</property>
</configuration>            <!-- 配置文件标签结束 -->
```

修改mapred-site.xml

```xml
<configuration>              <!-- 配置文件标签 -->
<property>                   <!-- 参数配置标签 -->
    <name>mapreduce.framework.name</name>        <!-- MapReduce 框架的名称 -->
    <value>yarn</value>               <!-- Hadoop 2.x 以上版本均使用YARN作为计算资源管理器 -->
</property>
<property>
 <name>mapreduce.application.classpath</name>	 	   <!--hadoop3.x必须配置,否则运行mr报错--> <value>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>
</property>
</configuration>            <!-- 配置文件标签结束 -->

```

修改yarn-site.xml

```xml
<configuration>              <!-- 配置文件标签 -->
<property>                   <!-- 参数配置标签 -->
    <name>yarn.resourcemanager.hostname</name>       <!-- YARN 资源管理器所在的节点 -->
    <value>master</value>                            <!-- YARN 资源管理器的节点名称，这里指定为名为 "master" 的节点 -->
</property>
<property>
    <name>yarn.nodemanager.aux-services</name>        <!-- NodeManager 中运行的附加服务 -->
    <value>mapreduce_shuffle</value>                 <!-- 指定为 MapReduce shuffle 服务 -->
</property>
</configuration>            <!-- 配置文件标签结束 -->

```

修改workers   **DataNode和NodeManager的所在机器**      (<u>hadoop2.x版本修改slaves</u>)

```sh
master
slave1
slave2
```

###### 4.分发集群

```shell
#分发jdk
[root@master /]# scp -r /opt/module/jdk slave1:/opt/module/
[root@master /]# scp -r /opt/module/jdk slave2:/opt/module/
#分发hadoop
[root@master /]# scp -r /opt/module/hadoop/ slave1:/opt/module/
[root@master /]# scp -r /opt/module/hadoop/ slave2:/opt/module/
#分发环境变量
[root@master /]# scp /etc/profile slave1:/etc/profile
[root@master /]# scp /etc/profile slave2:/etc/profile
#注意：环境变量分发完后，需要在对应主机上执行 source /etc/profile 命令，否则环境变量不生效。
```



###### 5.格式化hdfs文件系统

```shell
[root@master /]# hdfs namenode -format
```

查看格式化最后结果，出现 **successfully formatted** 等字符，即为成功。

###### 6.启动hadoop

第一种方式：单独启动hdfs和yarn

```shell
[root@master /]# start-dfs.sh 
[root@master /]# start-yarn.sh 
```

第二种方式：全部启动

```shell
[root@master /]# start-all.sh 
```

使用jps命令查看进程

```shell
[root@master /]# jps

#运行mr实例pi
hadoop jar /opt/module/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 10 10

#退出安全模式
hdfs dfsadmin -safemode leave
```

分别在三台主机上，以下是所有的进程分布：

master：NameNode、DataNode、ResourceManager、NodeManager

slave1：SecondaryNamenode、DataNode、NodeManager

slave2：DataNode、NodeManager

注意：如果hdfs格式化成功后，因错误问题要再次格式化hdfs系统，需要把集群所有主机hadoop目录下的tmp和logs目录删除才能再次格式化。

###### 三.动态添加删除 DataNode 和Nodemanager节点

###### 1.动态删除slave2的节点

修改master机器的worker文件,删除slave2这一行

```
master
slave1
```

分发这个文件至其他集群

在slave2的机器的shell分别关闭datanode和nodemanager

```shell
[root@master /]#hdfs --daemon stop datanode
[root@master /]#yarn --daemon stop nodemanager
```

在master的shell刷新hadoop节点

```shell
[root@master /]#hdfs dfsadmin -refreshNodes
```

查看hadoop集群状态

```shell
[root@master /]#hdfs dfsadmin -report
```

datanode没有slave2即为删除成功

###### 2.动态添加slave2的节点

修改master机器的worker文件,添加slave2这一行

```
master
slave1
slave2
```

分发这个文件至其他集群

在slave2的机器的shell分别启动datanode和nodemanager

```shell
[root@master /]#hdfs --daemon start datanode
[root@master /]#yarn --daemon start nodemanager
```

在master的shell刷新hadoop节点

```shell
[root@master /]#hdfs dfsadmin -refreshNodes
```

查看hadoop集群状态

```shell
[root@master /]#hdfs dfsadmin -report
```

datanode有slave2即为添加成功

###### 四.配置安装zookeeper

###### 1.解压Zookeeper

进入/opt/software目录

```shell
[root@master software]# tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/
```

解压完成后，进入/opt/module目录下，修改目录名（便于配置路径）

```shell
[root@master module]# mv apache-zookeeper-3.5.7-bin/ zookeeper
```

###### 2.配置环境变量

进入/etc/profile文件,在文件中增加ZK_HOME环境变量

```shell
export JAVA_HOME=/opt/module/jdk
export HADOOP_HOME=/opt/module/hadoop
export ZK_HOME=/opt/module/zookeeper
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin
```

保存后，需要执行source命令

```shell
[root@master module]# source /etc/profile
```

###### 3.修改配置文件

进入/opt/module/zookeeper/conf目录

复制zoo_sample.cfg文件 并命名为 zoo.cfg

```shell
[root@master conf]# cp zoo_sample.cfg zoo.cfg
```

修改zoo.cfg文件

```shell
#修改data存储目录
dataDir=/opt/module/zookeeper/zkdata
#在文件底部添加
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888
```

###### 4.创建myid文件

由于/opt/data/zookeeper/zkdata目录并没有创建，所以要新建此目录

```shell
[root@master /]# mkdir -p /opt/module/zookeeper/zkdata
```

进入/opt/data/zookeeper/zkdata目录，执行以下命令

```shell
[root@master zkdata]# echo 1 > myid
```

###### 5.分发集群

```shell
#将zookeeper分发至集群
[root@master /]# scp -r /opt/module/zookeeper/ slave1:/opt/module/
[root@master /]# scp -r /opt/module/zookeeper/ slave2:/opt/module/
#将环境变量分发至集群
[root@master /]# scp /etc/profile slave1:/etc/profile
[root@master /]# scp /etc/profile slave2:/etc/profile
#注意：环境变量分发完后，需要在对应主机上执行 source /etc/profile 命令，否则环境变量不生效。

#注意：分别在slave1、slave2修改myid的内容为 2、3
```

###### 6.启动Zookeeper

```shell
[root@master /]# zkServer.sh start
#分别在三台主机上执行上述命令
#切记检查防火墙
```

###### 7.查看Zookeeper状态

```shell
#至少启动两台以上才可以看到状态
#可以在三台主机上执行以下命令查看状态
[root@master /]# zkServer.sh status            
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost.
Mode: follower
[root@master /]# 
```





# 模块D：Hive数据仓库安装

个人需要知道和理解：

```
 Hive 数据仓库的元数据管理
 Hive 配置文件与参数的意义
 HQL 语句与 SQL 语句的区别
 Hive 服务端和客户端
```


个人应能够：

```
 掌握 Hive 的安装与配置
 能够配置使用 MySQL 管理 Hive 元数据
 能够配置 Hive 支持多客户端并启动 Hive 服务
 能够使用 HQL
```

## 主要知识和技能点:

```
安装数据库、Hive 基础环境配置、配置 Hive 元数据至 MySQL、配置Hive 客户端、启动 Hive 等
```

###### 一.安装配置Hive

hive依赖与mysql和hadoop,请配置hive前检查两者均无问题

###### 1.解压Hive

进入/opt/software目录,将hive解压到/opt/module目录下，并改名为hive

###### 2.配置环境变量

在/etc/profile文件添加

```shell
export HIVE_HOME=/opt/module/hive
#在PATH变量末尾追加,此处并不是换行
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$HIVE_HOME/bin
```

别忘记执行source命令

###### 3.修改配置文件

进入/opt/module/hive/目录

修改hive-env.sh 

```shell
HADOOP_HOME=/opt/module/hadoop
```

新建hive-site.xml文件，并添加内容如下

```xml
<configuration>
<property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
</property>
<property>
        <name>javax.jdo.option.ConnectionDriverName</name>
    <!--mysql8驱动名为com.mysql.cj.jdbc.Driver-->
        <value>com.mysql.jdbc.Driver</value>
</property>
<property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
</property>
<property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
</property>
</configuration>
```

###### 4.替换并添加包

进入/opt/module/hive/lib/目录

将hadoop里的guava包替换到此处

```shell
[root@master lib]# cp /opt/module/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar ./
#将原本的guava删除
[root@master lib]# rm -rf guava-19.0.jar
```

将mysql的驱动包复制到此处

```shell
[root@master lib]# cp /opt/software/mysql-connector-java-5.1.37.jar ./
```

###### 5.初始化

执行命令

```shell
[root@master lib]# schematool -dbType mysql -initSchema
```

结果最后一行为：schemaTool completed 	

使用hive命令，即可进入命令行。

###### 6.配置远程

修改hive配置文件hive-site.xml追加下面内容

```xml
<property>
        <name>hive.server2.thrift.port</name>
    <value>10000</value>
</property>
<property>
        <name>hive.server2.thrift.bind.host</name>
    <value>master</value>
</property>
```

修改hadoop配置文件core-site.xml.在最外层标签内追加下面内容

```xml
<property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
</property>
<property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
</property>

```

重启hadoop集群

```shell
[root@master lib]#stop-all.sh
[root@master lib]#start-all.sh
```

启动hiveserver2服务

```shell
[root@master lib]#hiveserver2
```

复制master,使用beeline连接hive

```mysql
[root@master lib]#beeline
beeline> !connect jdbc:hive2://master:10000
#输入mysql用户名
#输入mysql密码
```

完成远程配置

###### 二.常用HQL

HQL是Hive Query Language的缩写，它类似于SQL，但是针对的是Hadoop生态系统中的Hive。

以下是常用的HQL命令：

```mysql
1. SELECT：用于查询表格中的数据。
2. FROM：用于指定要查询的表格。
3. WHERE：用于指定查询条件，筛选出符合条件的数据。
4. GROUP BY：用于将数据按照指定的列进行分组。
5. ORDER BY：用于指定数据排序的顺序和方式。
6. JOIN：用于将两个或多个表格中的数据连接起来。
7. UNION：用于将两个或多个查询语句的结果组合成一个结果集。
8. INSERT INTO：用于向表格中插入新的数据。
9. CREATE TABLE：用于创建新的表格。
10. ALTER TABLE：用于修改已有的表格结构。
11. DROP TABLE：用于删除表格。
12. DESCRIBE：用于查看表的元数据信息，包括列名、列类型和列注释等。
13. SHOW DATABASES：用于查看当前Hive中存在的所有数据库。
14. USE：用于指定使用哪个数据库。
15. EXPLAIN：用于显示查询语句执行的详细信息。
16. PARTITION BY：用于将数据按照指定列进行分区，以便更快地查询和处理数据。
17. CLUSTER BY：用于将数据按照指定列进行聚类，以便更好地组织数据并提高查询效率。
18. DISTRIBUTED BY：用于将数据按照指定列进行分布，以便更好地在Hadoop集群上分布式计算。
19. SORT BY：用于按照指定列对数据进行排序。
20. LATERAL VIEW：用于在查询过程中展开复杂类型的数据结构，例如数组或结构体。
21. WINDOWING：用于在查询语句中定义窗口函数，以便对数据进行滑动窗口计算。
22. DISTRIBUTE BY：用于指定数据的分布方式，以便更好地在Hadoop集群上进行并行计算。
23. SORTED BY：用于指定数据的排序方式，以便更好地组织和处理数据。
24. SELCT DISTINCT：用于从表格中筛选出独特的数据值。
25. CASE WHEN：用于根据条件执行不同的操作，相当于if-else语句。
26. CAST：用于将一种数据类型转换为另一种数据类型。
27. SHOW TABLES：用于查看当前数据库下所有的表格。
28. CREATE VIEW：用于创建虚拟表格，以便更高效地查询数据。
29. DROP VIEW：用于删除虚拟表格。
30. LOAD DATA INPATH：用于从本地文件系统或HDFS中导入数据到Hive表格中
31. TRUNCATE TABLE：用于清空表格中的所有数据，但是保留表格结构和元数据信息。
32. CREATE DATABASE：用于创建新的数据库。
33. DROP DATABASE：用于删除指定的数据库，包括其中的所有表格和数据。
34. ALTER VIEW：用于修改已有的虚拟表格结构。
35. SET：用于设置Hive的各种配置参数。
36. ADD JAR：用于将本地JAR包导入到Hive中以便执行UDF等自定义函数。
37. CREATE FUNCTION：用于创建自定义的函数，以便在Hive中使用。
38. DROP FUNCTION：用于删除自定义的函数。
39. ANALYZE TABLE：用于对表格进行分析，包括计算统计信息和存储元数据等。
40. EXPORT TABLE：用于将表格中的数据导出到其他数据源或文件系统。
```



这是一些基础的用法,更多用法请自行search

# 模块E：Spark分析平台搭建

个人需要知道和理解：

```
 Scala 语言与 Java 语言的区别
 Spark 计算引擎的原理
 RDD 模型、DataFrame 模型的理解
 Spark 集群配置文件中的参数与意义
 Spark Core、Spark SQL 分析数据的基本步骤
```


个人应能够：

```
 掌握 Scala 语言的使用
 掌握 Spark 集群的搭建
 掌握 Spark Core 中常用的 Transform 和 Action 算子的使用
 掌握 Spark SQL 中 RDD 转 DataFrame 的操作
 掌握 Spark 读写数据的操作
```

## 主要知识的技能点:

```
Spark 安装包解压、配置环境变量、配置 Spark 配置文件、修改slaves 配置文件、开启集群并验证等
```

###### 一.安装配置spark

spark依赖于hadoop.zookeeper

###### 1.解压spark

进入/opt/software目录,将spark解压到/opt/module目录下，并改名为spark

###### 2.配置环境变量

在/etc/profile文件添加

```shell
export SPARK_HOME=/opt/module/spark
#在PATH变量末尾追加
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME/bin
```

别忘记执行source命令

###### 3.修改配置文件

进入/opt/module/spark/conf/目录

修改spark-env.sh文件（先复制spark-env.sh.template再改名）

```shell
#添加java环境变量
export JAVA_HOME=/opt/module/jdk
#指定Master的IP
export SPARK_MASTER_HOST=master
#指定Master的端口
export SPARK_MASTER_PORT=7077
#Hadoop配置文件
YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop
```

修改workers文件（先复制workers.template再改名）

```shell
slave1
slave2
```

###### 4.分发集群

```shell
#将zookeeper分发至集群
[root@master /]# scp -r /opt/module/spark/ slave1:/opt/module/
[root@master /]# scp -r /opt/module/spark/ slave2:/opt/module/
#将环境变量分发至集群
[root@master /]# scp /etc/profile slave1:/etc/profile
[root@master /]# scp /etc/profile slave2:/etc/profile
#注意：环境变量分发完后，需要在对应主机上执行 source /etc/profile 命令，否则环境变量不生效。
```

###### 5.启动Spark

启动命令为：start-all.sh

因为命令与hadoop的命令重复，所以只能在/opt/module/spark/sbin/目录下使用：

```shell
[root@master bin]# ./start-all.sh
```

进程分布：

master：Master

slave1：Worker

slave2：Worker

###### 6.Spark on yarn模式

进入/opt/module/hadoop/etc/hadoop/目录下：

修改hadoop的配置文件yarn-site.xml：

```xml
<!--在configuration标签里追加如下内容-->
<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>
<property>
	<name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
```

修改完后需要分发集群，

再次修改spark-env.sh文件

```shell
#添加java环境变量
export JAVA_HOME=/opt/module/jdk

#将这两个注释掉
#export SPARK_MASTER_HOST=master
#export SPARK_MASTER_PORT=7077

#添加yarn的配置文件路径
YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop
```

修改后分发集群

注意：重启yarn或hadoop集群 ，yarn模式不需要启动spark

测试命令：

spark-submit --master yarn --class org.apache.spark.examples.SparkPi  $SPARK_HOME/examples/jars/spark-examples_2.12-3.1.1.jar

（jar包版本可自行切换）



# 模块F：大数据集群应用与数据分析

个人需要知道和理解：

```

 Hadoop 集群中的组件与各自的职责
 MapReduce 计算引擎分析数据的基本步骤
 Hive 数据仓库与 Hadoop 的关系
 HQL 语言
 Spark 计算引擎中的 Transform 与 Action 算子
```

个人应能够：

```
 理解数据集中的字段与各自代表的意义
 掌握 HDFS 常用的操作
 掌握 Java、Scala 语言
 掌握 Hadoop、Hive、Spark 的启动方式
 掌握 MapReduce 计算框架的编程规范,能够编写相应的 MR 程序完成数据的离线分析
 能够使用 HQL 语句完成对应的数据离线分析任务
 能够使用 Scala 编写程序使用 Spark 引擎完成数据的离线
```

## 主要知识的技能点:

```
使用 Hadoop 中的 MapReduce 计算框架、Hive 数据仓库中的 HQL 语言、Spark 计算引擎完成日志类数据的离线分析等
```

