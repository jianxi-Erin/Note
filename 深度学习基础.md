# 深度学习基础

## 环境搭建
1. 安装Python

2. 安装Anaconda(miniconda)
```sh
# 使用conda管理虚拟环境

# 创建虚拟环境
conda create -n `env-name` `python=版本` [-c 镜像地址]
# 删除建虚拟环境
conda remove -n `env-name` --all
# 添加源
# channels:
#   - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
#   - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
#   - https://mirrors.ustc.edu.cn/anaconda/pkgs/free
#   - https://mirrors.ustc.edu.cn/anaconda/pkgs/main

conda config --add channels 通道地址
# 删除源
conda config --remove channels 通道地址
# 查看源
conda config --show channels
# 安装时显示安装源
conda config --set show_channel_urls yes


# 查看环境列表
conda env list

# 配置powershell正确显示(环境名)
conda init powershell
conda config --set auto_activate_base false



#激活某个虚拟环境(*)
conda activate env_name
# 退出某个虚拟环境(*)
conda deactivate

# 使用pip安装/卸载虚拟环境包
pip install/uninstall 包[=版本号] [-i 源链接]

pip install -r "requirements.txt文件" [-i 源]
# 更新包
pip install --upgrade 包
#查看安装的包 
pip list

# conda install/uninstall 包名[=版本号] [-i 源链接]
```
3. 安装CUDA驱动/运行环境
```sh
# 1. 确定显卡为Nvidia系列
Windows: 使用组合键`Ctrl+Shift+ESC`打开任务管理器，选择`性能`->`GPU1`查看显卡型号

# 2. 安装显卡驱动
官网安装

# 3. 安装CUDA
确定cuda最高版本  nvidia-smi # 或 打开Nvidia控制面板->系统信息->组件
安装 conda install cuda -c nvidia/label/cuda-版本 #或 官网安装
检查安装是否成功 nvcc --version 

# 4. 安装cuDNN
确定cudnn版本 conda search cudnn --info # 或 官网查询
安装 conda install cudaa=版本 
验证安装 nvidia-smi 

```
4. Windows安装pytorch和cudnn
```url 
安装地址:
https://pytorch.org/get-started/locally/
```
```sh
# 激活conda环境


# conda安装 -c 镜像源
#cpu版本
conda install pytorch torchvision torchaudio cpuonly -c pytorch
# cuda版本 # 如果未安装cuda和cudnn会自动安装
conda install pytorch torchvision torchaudio pytorch-cuda=版本号 -c pytorch -c nvidia


# pip安装

#cpu版本
pip3 install torch torchvision torchaudio
# cuda版本  #如果未安装cuda和cudnn会自动安装
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu版本号

```
5. 创建Tensorflow环境
```python
conda create -n tensorflow_env python=3.9
conda activate tensorflow_env
pip install tensorflow==1.13.1
pip install keras==2.2.4
pip install matplotlib
pip install opencv-python
pip install numpy
```

6. 常用数据集
```
飞桨数据集 https://aistudio.baidu.com/datasetoverview
kaggle数据集 https://www.kaggle.com/datasets
```

## 常见名词

- **特征（Feature）**：用于描述数据样本的属性或属性值，是机器学习模型的输入变量。例如，在房价预测中，房屋的面积、房间数量等都是特征。
- **标签（Label）**：与特征相对应的输出变量，是模型需要预测的目标值。例如，在房价预测中，房屋的价格就是标签。
- **数据集（Dataset）**：用于训练和测试模型的数据集合，通常包含特征和标签。
- **训练集（Training Set）**：用于训练模型的数据集，模型通过学习训练集中的特征和标签之间的关系来构建模型。
- **验证集（Validation Set）**：用于调整模型超参数和评估模型性能的数据集，帮助防止拟过合。
- **测试集（Test Set）**：用于最终评估模型性能的数据集，模型在测试集上的表现通常代表其在实际应用中的性能。
- **模型（Model）**：通过学习数据集中的特征和标签之间的关系而构建的数学模型，用于对新的数据进行预测或分类。
- **算法（Algorithm）**：用于构建模型的方法或过程，例如线性回归、决策树、神经网络等。
- **过拟合（Overfitting）**：模型在训练集上表现很好，但在测试集上表现较差的现象，通常是由于模型过于复杂导致的。
- **欠拟合（Underfitting）**：模型在训练集上表现较差的现象，通常是由于模型过于简单导致的。
- **损失函数（Loss Function）**：用于衡量模型预测值与真实值之间差异的函数，模型的目标是通过优化损失函数来提高预测精度。
- **优化算法（Optimization Algorithm）**：用于最小化损失函数的算法，例如梯度下降法。
- **超参数（Hyperparameter）**：在模型训练之前需要手动设置的参数，例如学习率、迭代次数等。
- **参数（Parameter）**：模型在训练过程中自动学习的参数，例如线性回归中的权重和偏置。
- **正则化（Regularization）**：通过在损失函数中添加惩罚项来防止过拟合的技术，常见的有L1正则化和L2正则化。
- **监督学习（Supervised Learning）**：模型通过学习带标签的数据来构建模型的学习方式，例如分类和回归。
- **无监督学习（Unsupervised Learning）**：模型学习通过无标签的数据来发现数据中的结构或模式的学习方式，例如聚类和降维。
- **半监督学习（Semi-supervised Learning）**：模型通过学习少量带标签的数据和大量无标签的数据来构建模型的学习方式。
- **强化学习（Reinforcement Learning）**：模型通过与环境的交互来学习最优策略的学习方式，通过奖励和惩罚来调整模型的行为。
- **分类（Classification）**：监督学习的一种任务，目标是将数据划分为不同的类别，例如垃圾邮件检测。
- **回归（Regression）**：监督学习的一种任务，目标是预测一个连续的数值，例如房价预测。
- **聚类（Clustering）**：无监督学习的一种任务，目标是将数据划分为不同的簇，使得簇内的数据相似度高，簇间的相似度低。
- **降维（Dimensionality Reduction）**：无监督学习的一种任务，目标是将高维数据映射到低维空间，同时保留数据的主要信息，例如主成分分析（PCA）。
- **准确率（Accuracy）**：分类模型中，预测正确的样本数占总样本数的比例。
- **召回率（Recall）**：在分类模型中，真正例被正确预测的比例。
- **精确率（Precision）**：在分类模型中，被预测为正例样本的中真正例的比例。
- **F1分数（F1 Score）**：精确率和召回率的调和平均值，用于综合评估分类模型的性能。
- **混淆矩阵（Confusion Matrix）**：用于评估分类模型性能的矩阵，包含真正例、假正例、真负例和假负例的数量。
- **ROC曲线（Receiver Operating Characteristic Curve）**：用于评估分类模型性能的曲线，横轴为假正例率，纵轴为真正例率。
- **AUC值（Area Under the ROC Curve）**：ROC曲线下的面积，用于衡量分类模型的性能，值越大表示模型性能越好。
- **混淆矩阵（Confusion Matrix）**：用于评估分类模型性能的矩阵，记录了模型预测结果与真实标签之间的关系。通常包含以下四个基本指标：
    - **真正例（True Positive, TP）**：模型正确预测为正类的样本数。
    - **假正例（False Positive, FP）**：模型错误预测为正类的样本数。
    - **真负例（True Negative, TN）**：模型正确预测为负类的样本数。
    - **假负例（False Negative, FN）**：模型错误预测为负类的样本数。
- **准确率（Accuracy）**：模型预测正确的样本数占总样本数的比例，计算公式为：
    $$
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    $$
- **精确率（Precision）**：被预测为正类的样本中，真正为正类的比例，计算公式为：
    $$
    \text{Precision} = \frac{TP}{TP + FP}
    $$
- **召回率（Recall）**：所有正类样本中，被正确预测为正类的比例，计算公式为：
    $$
    \text{Recall} = \frac{TP}{TP + FN}
    $$
- **F1分数（F1 Score）**：精确率和召回率的调和平均值，用于综合评估分类模型的性能，计算公式为：
    $$
    \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    $$
- **F2分数（F2 Score）**：更强调召回率的指标，计算公式为：
    $$
    \text{F2 Score} = 5 \times \frac{\text{Precision} \times \text{Recall}}{4 \times \text{Precision} + \text{Recall}}
    $$



## 机器学习常用模型
### 线性回归(LinearRegression)
- h(x)=w0+w1 * x1+w2 * x2+...
- 线性回归是机器学习中一个最基本的模型，它通过学习训练特征数据(w1,w2...)，来预测目标变量(h(x))。假设目标变量与特征之间存在线性关系，即可以用一个线性方程来表示这种关系。
- 误差项w0: 预测值与真实值之间的误差,是独立的,并具有相同的分,均值为0,方差值为参数平方的高斯分布
- 独立: 数据项直接不会互相影响
- 同分布: 数据尽可能来自同一个地方
- 高斯(正态)分布: 数据小范围浮动几率大,数据大范围浮动几率小
```python
from sklearn.linear_model import LinearRegression

# 创建线性回归模型对象
model = LinearRegression(
    fit_intercept=True,  # 是否计算截距（即 y = wx + b 中的 b）。默认为 True。
    copy_X=True,        # 是否复制 X（输入数据）。默认为 True。如果为 False，可能会覆盖原始数据。
    n_jobs=None,        # 用于计算的作业数。默认为 None，表示使用 1 个作业。如果为 -1，则使用所有可用的 CPU 核心。
    positive=False      # 是否强制系数为正。默认为 False。如果为 True，则强制系数为非负。
)

# 参数解释：
# - fit_intercept: 是否计算模型的截距。如果设置为 False，则模型不会计算截距（即 b = 0）。
# - copy_X: 是否复制输入数据 X。如果设置为 False，可能会在拟合过程中覆盖原始数据。
# - n_jobs: 用于并行计算的作业数。如果设置为 -1，则使用所有可用的 CPU 核心。
# - positive: 是否强制模型的系数为非负。如果设置为 True，则模型的系数将是非负的。

# 拟合模型
model.fit(X_train, y_train)

# 预测
predictions = model.predict(X_test)
print(predictions)  # 输出预测结果
```
#### 梯度下降优化算法
- 学习率(步长): 学习率是梯度下降算法中的一个重要参数，它控制着每次参数更新的步长。学习率越大，每次更新参数的步长越大，迭代次数越少，但同时可能会导致模型过拟合。学习率越小，每次更新参数的步长越小，迭代次数越多，但同时可能会导致模型欠拟合。
- 梯度下降的作用
梯度下降是一种迭代优化算法，用于找到目标函数的最小值。在机器学习中，目标函数通常是损失函数（如均方误差或交叉熵损失），而梯度下降通过调整模型参数来最小化这些损失函数。
- 梯度下降的工作原理
梯度下降的核心思想是利用目标函数的梯度（即导数）来指导参数的更新方向

- 具体步骤如下：
初始化参数：选择一个初始的参数值（通常是随机初始化）。
计算梯度：计算目标函数在当前参数下的梯度（即导数）。
更新参数：根据梯度的方向更新参数，使目标函数的值减小。
迭代：重复上述步骤，直到目标函数的值收敛到最小值。
```python
import numpy as np

# 生成示例数据
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 添加偏置项（intercept）
X_b = np.c_[np.ones((100, 1)), X]

# 初始化参数
theta = np.random.randn(2, 1)

# 超参数
learning_rate = 0.1
n_iterations = 1000

# 梯度下降
for iteration in range(n_iterations):
    gradients = 2 / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - learning_rate * gradients

# 输出最终参数
print("最优参数：", theta)

# 预测
X_new = np.array([[0], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new]
y_pred = X_new_b.dot(theta)
print("预测结果：", y_pred)
```
### 逻辑回归（Logistic Regression）
- 逻辑回归是机器学习中一个最基本的模型，是一种广泛应用于二分类问题的机器学习算法，尽管名字中有“回归”二字，但它实际上是一种分类算法。
```python
from sklearn.linear_model import LogisticRegression

# 创建逻辑回归模型对象
model = LogisticRegression(
    penalty='l2',              # 正则化类型，可选 'l1', 'l2', 'elasticnet', 'none'。默认为 'l2'。
    dual=False,                # 是否使用对偶形式。默认为 False。适用于样本数 > 特征数的情况。
    tol=1e-4,                  # 优化算法的收敛阈值。默认为 1e-4。
    C=1.0,                     # 正则化强度的倒数。C 越小，正则化越强。默认为 1.0。
    fit_intercept=True,        # 是否计算截距。默认为 True。
    intercept_scaling=1,       # 当使用正则化且 fit_intercept=True 时，缩放截距。默认为 1。
    class_weight=None,         # 类别权重。可以设置为 'balanced' 或字典形式。默认为 None。
    random_state=None,         # 随机种子，用于控制随机性。默认为 None。
    solver='lbfgs',            # 优化算法。可选 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'。默认为 'lbfgs'。
    max_iter=100,              # 最大迭代次数。默认为 100。
    multi_class='auto',        # 多分类策略。可选 'auto', 'ovr', 'multinomial'。默认为 'auto'。
    verbose=0,                 # 日志输出详细程度。默认为 0（不输出）。
    warm_start=False,          # 是否使用上一次训练的结果初始化。默认为 False。
    n_jobs=None,               # 并行计算作业数。默认为 None。
    l1_ratio=None              # Elastic-Net 混合参数（仅当 penalty='elasticnet' 时有效）。默认为 None。
)

# 参数解释：
# - penalty: 正则化类型。'l1' 和 'l2' 分别对应 L1 和 L2 正则化，'elasticnet' 是 L1 和 L2 的组合。
# - dual: 是否使用对偶形式。仅当 penalty='l2' 且 solver='liblinear' 时有效。
# - tol: 优化算法的收敛阈值。损失函数的变化小于该值时停止迭代。
# - C: 正则化强度的倒数。C 越小，正则化越强。
# - fit_intercept: 是否计算截距。
# - intercept_scaling: 当使用正则化且 fit_intercept=True 时，缩放截距。
# - class_weight: 类别权重。可以设置为 'balanced' 或字典形式，用于处理类别不平衡问题。
# - random_state: 随机种子，用于控制随机性。
# - solver: 优化算法。'lbfgs' 是默认算法，适用于大多数情况。
# - max_iter: 最大迭代次数。
# - multi_class: 多分类策略。'ovr' 是“一对多”，'multinomial' 是“多项式”。
# - verbose: 日志输出详细程度。
# - warm_start: 是否使用上一次训练的结果初始化。
# - n_jobs: 并行计算作业数。
# - l1_ratio: Elastic-Net 混合参数（仅当 penalty='elasticnet' 时有效）。

# 示例数据
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 加载数据集
X, y = load_iris(return_X_y=True)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 拟合模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 输出预测结果
print("预测结果:", y_pred)
```
### K近邻算法（K-Nearest Neighbors, KNN）
是一种简单而有效的分类和回归算法。它的核心思想是：对于一个未知样本，找到训练集中与其最接近的 \( k \) 个样本，然后根据这 \( k \) 个样本的类别（分类问题）或值（回归问题）来预测未知样本的类别或值。

在 `scikit-learn` 中，KNN 的实现由 `KNeighborsClassifier`（用于分类）和 `KNeighborsRegressor`（用于回归）提供。

**KNN 分类模型的参数**
```python
from sklearn.neighbors import KNeighborsClassifier

# 创建 KNN 分类模型对象
model = KNeighborsClassifier(
    n_neighbors=5,            # 使用的邻居数，默认为 5。
    weights='uniform',        # 权重函数。可选 'uniform'（等权重）或 'distance'（按距离加权）。默认为 'uniform'。
    algorithm='auto',         # 计算最近邻的算法。可选 'auto', 'ball_tree', 'kd_tree', 'brute'。默认为 'auto'。
    leaf_size=30,             # 传递给 BallTree 或 KDTree 的叶子大小。默认为 30。
    p=2,                      # 距离度量参数。p=1 为曼哈顿距离，p=2 为欧氏距离。默认为 2。
    metric='minkowski',       # 距离度量类型。默认为 'minkowski'（闵可夫斯基距离）。
    metric_params=None,       # 距离度量的额外参数。默认为 None。
    n_jobs=None               # 并行计算作业数。默认为 None。
)

# 参数解释：
# - n_neighbors: 使用的邻居数。k 值越小，模型越复杂；k 值越大，模型越简单。
# - weights: 权重函数。'uniform' 表示所有邻居的权重相等，'distance' 表示权重与距离成反比。
# - algorithm: 计算最近邻的算法。'auto' 自动选择最佳算法，'ball_tree' 和 'kd_tree' 适用于高维数据，'brute' 适用于低维数据。
# - leaf_size: 传递给 BallTree 或 KDTree 的叶子大小。影响树的构建和查询速度。
# - p: 距离度量参数。p=1 为曼哈顿距离，p=2 为欧氏距离。
# - metric: 距离度量类型。默认为 'minkowski'（闵可夫斯基距离）。
# - metric_params: 距离度量的额外参数。
# - n_jobs: 并行计算作业数。

# 示例数据
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 加载数据集
X, y = load_iris(return_X_y=True)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 拟合模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 输出预测结果
print("预测结果:", y_pred)
```
**KNN 回归模型的参数**
KNN 回归模型的参数与分类模型几乎相同，唯一的区别是它的目标值是连续值而不是类别。

```python
from sklearn.neighbors import KNeighborsRegressor

# 创建 KNN 回归模型对象
model = KNeighborsRegressor(
    n_neighbors=5,            # 使用的邻居数，默认为 5。
    weights='uniform',        # 权重函数。可选 'uniform'（等权重）或 'distance'（按距离加权）。默认为 'uniform'。
    algorithm='auto',         # 计算最近邻的算法。可选 'auto', 'ball_tree', 'kd_tree', 'brute'。默认为 'auto'。
    leaf_size=30,             # 传递给 BallTree 或 KDTree 的叶子大小。默认为 30。
    p=2,                      # 距离度量参数。p=1 为曼哈顿距离，p=2 为欧氏距离。默认为 2。
    metric='minkowski',       # 距离度量类型。默认为 'minkowski'（闵可夫斯基距离）。
    metric_params=None,       # 距离度量的额外参数。默认为 None。
    n_jobs=None               # 并行计算作业数。默认为 None。
)

# 示例数据
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# 生成回归数据集
X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 拟合模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 输出预测结果
print("预测结果:", y_pred)
```
### 决策树（Decision Tree）
是一种基于树结构的机器学习算法，既可以用于分类任务，也可以用于回归任务。它的核心思想是通过递归地划分数据集，构建一棵树，每个内部节点表示一个特征的分裂规则，每个叶节点表示一个类别（分类问题）或值（回归问题）。

决策树分类模型的参数
```python
from sklearn.tree import DecisionTreeClassifier

# 创建决策树分类模型对象
model = DecisionTreeClassifier(
    criterion='gini',          # 分裂标准。可选 'gini'（基尼系数）或 'entropy'（信息增益）。默认为 'gini'。
    splitter='best',           # 分裂策略。可选 'best'（最佳分裂）或 'random'（随机分裂）。默认为 'best'。
    max_depth=None,            # 树的最大深度。默认为 None，表示不限制深度。
    min_samples_split=2,       # 分裂内部节点所需的最小样本数。默认为 2。
    min_samples_leaf=1,        # 叶节点所需的最小样本数。默认为 1。
    min_weight_fraction_leaf=0.0,  # 叶节点所需的最小权重比例。默认为 0.0。
    max_features=None,         # 寻找最佳分裂时考虑的最大特征数。默认为 None，表示考虑所有特征。
    random_state=None,         # 随机种子，用于控制随机性。默认为 None。
    max_leaf_nodes=None,       # 最大叶节点数。默认为 None，表示不限制。
    min_impurity_decrease=0.0, # 分裂所需的最小不纯度减少量。默认为 0.0。
    class_weight=None,         # 类别权重。可以设置为 'balanced' 或字典形式。默认为 None。
    ccp_alpha=0.0              # 用于剪枝的复杂度参数。默认为 0.0。
)

# 参数解释：
# - criterion: 分裂标准。'gini' 使用基尼系数，'entropy' 使用信息增益。
# - splitter: 分裂策略。'best' 选择最佳分裂，'random' 选择随机分裂。
# - max_depth: 树的最大深度。限制树的深度可以防止过拟合。
# - min_samples_split: 分裂内部节点所需的最小样本数。
# - min_samples_leaf: 叶节点所需的最小样本数。
# - min_weight_fraction_leaf: 叶节点所需的最小权重比例。
# - max_features: 寻找最佳分裂时考虑的最大特征数。
# - random_state: 随机种子，用于控制随机性。
# - max_leaf_nodes: 最大叶节点数。
# - min_impurity_decrease: 分裂所需的最小不纯度减少量。
# - class_weight: 类别权重，用于处理类别不平衡问题。
# - ccp_alpha: 用于剪枝的复杂度参数。

# 示例数据
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 加载数据集
X, y = load_iris(return_X_y=True)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 拟合模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 输出预测结果
print("预测结果:", y_pred)
```
决策树回归模型的参数
```python
from sklearn.tree import DecisionTreeRegressor

# 创建决策树回归模型对象
model = DecisionTreeRegressor(
    criterion='squared_error',  # 分裂标准。可选 'squared_error'（均方误差）或 'friedman_mse'（改进的均方误差）。默认为 'squared_error'。
    splitter='best',           # 分裂策略。可选 'best'（最佳分裂）或 'random'（随机分裂）。默认为 'best'。
    max_depth=None,            # 树的最大深度。默认为 None，表示不限制深度。
    min_samples_split=2,       # 分裂内部节点所需的最小样本数。默认为 2。
    min_samples_leaf=1,        # 叶节点所需的最小样本数。默认为 1。
    min_weight_fraction_leaf=0.0,  # 叶节点所需的最小权重比例。默认为 0.0。
    max_features=None,         # 寻找最佳分裂时考虑的最大特征数。默认为 None，表示考虑所有特征。
    random_state=None,         # 随机种子，用于控制随机性。默认为 None。
    max_leaf_nodes=None,       # 最大叶节点数。默认为 None，表示不限制。
    min_impurity_decrease=0.0, # 分裂所需的最小不纯度减少量。默认为 0.0。
    ccp_alpha=0.0              # 用于剪枝的复杂度参数。默认为 0.0。
)

# 示例数据
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# 生成回归数据集
X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 拟合模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 输出预测结果
print("预测结果:", y_pred)
```
### 支持向量机（Support Vector Machine, SVM）
是一种强大的监督学习算法，广泛用于分类和回归任务。它的核心思想是找到一个超平面，将不同类别的样本分开，并且最大化类别之间的边界（即“间隔”）。
SVC分类模型
```python
from sklearn.svm import SVC

# 创建 SVM 分类模型对象
model = SVC(
    C=1.0,                    # 正则化参数。C 越小，正则化越强。默认为 1.0。
    kernel='rbf',             # 核函数类型。可选 'linear', 'poly', 'rbf', 'sigmoid'。默认为 'rbf'。
    degree=3,                 # 多项式核函数的次数（仅当 kernel='poly' 时有效）。默认为 3。
    gamma='scale',            # 核函数的系数。可选 'scale', 'auto' 或具体数值。默认为 'scale'。
    coef0=0.0,                # 核函数中的独立项（仅当 kernel='poly' 或 'sigmoid' 时有效）。默认为 0.0。
    shrinking=True,           # 是否使用启发式收缩。默认为 True。
    probability=False,        # 是否启用概率估计。默认为 False。
    tol=1e-3,                 # 优化算法的收敛阈值。默认为 1e-3。
    cache_size=200,           # 核函数缓存大小（以 MB 为单位）。默认为 200。
    class_weight=None,        # 类别权重。可以设置为 'balanced' 或字典形式。默认为 None。
    verbose=False,            # 是否启用详细输出。默认为 False。
    max_iter=-1,              # 最大迭代次数。-1 表示无限制。默认为 -1。
    decision_function_shape='ovr',  # 决策函数形状。可选 'ovr'（一对多）或 'ovo'（一对一）。默认为 'ovr'。
    break_ties=False,         # 是否打破平局（仅当 decision_function_shape='ovr' 时有效）。默认为 False。
    random_state=None         # 随机种子，用于控制随机性。默认为 None。
)

# 参数解释：
# - C: 正则化参数。C 越小，正则化越强。
# - kernel: 核函数类型。'linear' 是线性核，'poly' 是多项式核，'rbf' 是径向基核，'sigmoid' 是 sigmoid 核。
# - degree: 多项式核函数的次数（仅当 kernel='poly' 时有效）。
# - gamma: 核函数的系数。'scale' 表示 1 / (n_features * X.var())，'auto' 表示 1 / n_features。
# - coef0: 核函数中的独立项（仅当 kernel='poly' 或 'sigmoid' 时有效）。
# - shrinking: 是否使用启发式收缩。
# - probability: 是否启用概率估计。
# - tol: 优化算法的收敛阈值。
# - cache_size: 核函数缓存大小（以 MB 为单位）。
# - class_weight: 类别权重，用于处理类别不平衡问题。
# - verbose: 是否启用详细输出。
# - max_iter: 最大迭代次数。-1 表示无限制。
# - decision_function_shape: 决策函数形状。'ovr' 是“一对多”，'ovo' 是“一对一”。
# - break_ties: 是否打破平局（仅当 decision_function_shape='ovr' 时有效）。
# - random_state: 随机种子，用于控制随机性。

# 示例数据
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 加载数据集
X, y = load_iris(return_X_y=True)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 拟合模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 输出预测结果
print("预测结果:", y_pred)

```
SVR回归模型
```python
from sklearn.svm import SVR

# 创建 SVM 回归模型对象
model = SVR(
    kernel='rbf',             # 核函数类型。可选 'linear', 'poly', 'rbf', 'sigmoid'。默认为 'rbf'。
    degree=3,                 # 多项式核函数的次数（仅当 kernel='poly' 时有效）。默认为 3。
    gamma='scale',            # 核函数的系数。可选 'scale', 'auto' 或具体数值。默认为 'scale'。
    coef0=0.0,                # 核函数中的独立项（仅当 kernel='poly' 或 'sigmoid' 时有效）。默认为 0.0。
    tol=1e-3,                 # 优化算法的收敛阈值。默认为 1e-3。
    C=1.0,                    # 正则化参数。C 越小，正则化越强。默认为 1.0。
    epsilon=0.1,              # 控制回归模型的容忍度。默认为 0.1。
    shrinking=True,           # 是否使用启发式收缩。默认为 True。
    cache_size=200,           # 核函数缓存大小（以 MB 为单位）。默认为 200。
    verbose=False,            # 是否启用详细输出。默认为 False。
    max_iter=-1               # 最大迭代次数。-1 表示无限制。默认为 -1。
)

# 示例数据
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# 生成回归数据集
X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 拟合模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 输出预测结果
print("预测结果:", y_pred)
```

## 训练模型的步骤
训练模型的完整步骤：

问题定义：明确任务类型（分类、回归、聚类等）和评估指标。

数据准备：收集数据、处理缺失值、异常值、重复值。

特征工程：特征编码、标准化/归一化、特征选择、降维（如PCA）。

数据划分：将数据集划分为训练集、验证集和测试集（常用比例如 7:3 或 8:2）。

模型选择：根据任务类型选择算法（如线性回归、决策树、SVM）。

模型训练：在训练集上拟合模型，调整参数（如学习率、正则化系数）。

模型评估：在验证集上评估性能（如MSE、R²、准确率），分析过拟合/欠拟合。

模型调优：通过网格搜索、随机搜索或贝叶斯优化调整超参数。

最终评估：在测试集上测试模型泛化能力。

部署与监控：部署模型并持续监控性能。
