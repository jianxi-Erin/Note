# 大数据运维部署

## 模块A:操作系统基础环境

1.

```bash
[root@master /]# cd /root/
[root@master ~]# mkdir student
```

2.

```bash
[root@master ~]# cd student/
[root@master student]# date > qingjiao
```

3.

```bash
[root@master student]# cd ..
[root@master ~]# ln -s student college
```

4.

```bash
[root@master ~]# groupadd -g 1010 qingjiao
```

5.

```bash
[root@master ~]# useradd -m -g qingjiao qj
```

6.

```bash
#mysql文件夹必须先存在,如果不存在可以用mkdir mysql创建
[root@master ~]# tar -cvf mysqldata.tar mysql
```

7.

```bash
[root@master ~]# cd student/
[root@master student]# vim sum50.sh
```

```sh
sum=0
i=1

while [ $i -le 50 ]
do
    sum=$((sum + i))
    i=$((i + 1))
done

echo "$sum"
```

```bash
[root@master student]# chmod 777 sum50.sh
[root@master student]# ./sum50.sh
1275
```

## 模块B 数据库系统配置

### 1.安装mysql数据库
检查并卸载已安装的mysql和mariadb
```bash
#请自行上传mysql相关包到software
[root@master software]# rpm -qa|grep mysql
[root@master software]# rpm -qa | grep mariadb
[root@master software]# rpm -e --nodeps xxxxxxxxxxx
```

安装
````bash
#如缺少依赖请自行用yum安装
[root@master software]# rpm -ivh mysql-community-common-5.7.36-1.el7.x86_64.rpm
[root@master software]# rpm -ivh mysql-community-libs-5.7.36-1.el7.x86_64.rpm
[root@master software]# rpm -ivh mysql-community-client-5.7.36-1.el7.x86_64.rpm
[root@master software]# rpm -ivh mysql-community-server-5.7.36-1.el7.x86_64.rpm
#修改配置文件
[root@master software]# vim /etc/my.cnf
```
default-storage-engine=innodb
innodb_file_per_table
collation-server=utf8_general_ci
init-connect='SET NAMES utf8'
character-set-server=utf8
```
````

启动服务并检查状态
```bash
#启动服务
[root@master software]# systemctl start mysqld
#查看服务状态
[root@master software]# systemctl status mysqld
#active (running)启动成功
```
MySQL初始化配置
```bash
#输出临时密码
[root@master software]# cat /var/log/mysqld.log|grep password
#使用临时密码登录
[root@master software]# mysql -uroot -p

#设置密码安全策略
mysql> set global validate_password_policy=low;
Query OK, 0 rows affected (0.00 sec)
#设置密码最短为6位
mysql> set global validate_password_length = 6;
Query OK, 0 rows affected (0.00 sec)
#配置远程和本地登陆密码
mysql> ALTER USER 'root'@'localhost' IDENTIFIED BY '123456';
Query OK, 0 rows affected (0.01 sec)
mysql> grant all privileges on *.* to root@'%' identified by '123456' with grant option;
Query OK, 0 rows affected, 1 warning (0.00 sec)
#验证本地登录和远程登录
[root@master software]# mysql -uroot -p
[root@master software]# mysql -uroot -p -h 127.0.0.1
```

### 2. SQL数据操作

数据导入

```bash
#自行准备score.sql文件上传到/opt/software
#登录mysql并创建demo数据库
[root@master software]# mysql -uroot -p
mysql> create database demo;
Query OK, 1 row affected (0.01 sec)
#导入数据
mysql> source /opt/score.sql
```

csv文件导入可以使用navicat,如果用命令需要手动建表

存入视图table1

```sql
create view table1 as
select * from demo.score
limit 5,3;
```

存入视图table2

```sql
create view table2 as
select * from demo.score
where chinese=90;
```

存入视图table3

```mysql
create view table3 as
select class,name,math from demo.score 
where math>(select avg(math) from demo.score);
```



## 模块C:Hadoop集群部署

### 1.基础环境配置
修改主机名
```bash
[root@master /]# hostnamectl set-hostname master
[root@slave1 /]# hostnamectl set-hostname slave1
[root@slave2 /]# hostnamectl set-hostname slave2
```
修改hosts
````bash
#修改hosts 先自行查看三台机器ip
[root@master /]# vim /etc/hosts
```
172.22.0.x      master
172.22.0.x      slave1
172.22.0.x      slave2
```
#使用ping检验
````
时区设置
```bash
#查看时区列表并过滤上海
[root@master /]# timedatectl list -timezones | grep Shanghai
#设置时区为上海Asia/shanghai
[root@master /]# timedatectl set-timezone Asia/Shanghai
#查看时区
[root@master /]# timedatectl

#方法2
tzselect
5
9
1
yes
1
#复制打印信息到profile中
```
ntp时间同步
````bash
#ntp             需自行安装ntp,yum install安装
[root@master /]# vim /etc/ntp.conf
```
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server 127.127.1.0
fudge 127.127.1.0 stratum 10
```
#重启ntp服务
[root@master /]# systemctl restart ntpd
#同步
[root@slave1 /]# ntpdate -u master
[root@slave2 /]# ntpdate -u master
````
定时任务
````bash
#定时任务  yum install crontabs
#查看进程绝对路径
[root@slave1 /]# where ntpdate
#制定定时任务
[root@slave1 /]# crontab -e
```
#分 时 天 月 星期
*/30 10-17 * * * ntpdate -u master
```
#查看定时任务
crontab -e

[root@slave2 /]# crontab -e
```
*/30 10-17 * * * ntpdate -u master
```
````
免密
```bash
#免密登录
[root@master /]# ssh-keygen
[root@master /]# ssh-copy-id master
[root@master /]# ssh-copy-id slave1
[root@master /]# ssh-copy-id slave2
```
java安装
```bash
#jdk安装
[root@master /]# mkdir -p /usr/java
[root@master /]# cd /opt/software/
[root@master software]# tar -zxvf jdk-8u212-linux-x64.tar.gz -C /usr/java/
[root@master software]# mv /usr/java/jdk1.8.0_212/ /usr/java/jdk
[root@master software]# echo 'export JAVA_HOME=/usr/java/jdk1.8.0_212' >>/etc/profile
[root@master software]# echo 'export PATH=$PATH:$JAVA_HOME/bin' >>/etc/profile
[root@master software]# source /etc/profile
[root@master software]# java -version
#自行分发	
````

### 2. Zookeeper集群搭建
解压与环境变量
````bash
#zookeeper
[root@master usr]# mkdir -p /usr/zookeeper
[root@master usr]# cd /opt/software/
[root@master software]# tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /usr/zookeeper/
#环境变量
[root@master software]# echo 'export ZK_HOME=/usr/zookeeper/apache-zookeeper-3.5.7-bin' >> /etc/profile
[root@master software]# echo 'export PATH=$PATH:$ZK_HOME/bin' >> /etc/profile
[root@master software]# source /etc/profile
````
#配置
````bash
[root@master software]# cd /usr/zookeeper/apache-zookeeper-3.5.7-bin/
[root@master apache-zookeeper-3.5.7-bin]# mkdir zkdata
[root@master apache-zookeeper-3.5.7-bin]# mkdir zkdatalog
[root@master apache-zookeeper-3.5.7-bin]# cd conf/
[root@master conf]# mv zoo_sample.cfg zoo.cfg
[root@master conf]# vim zoo.cfg
```
dataDir=/usr/zookeeper/apache-zookeeper-3.5.7-bin/zkdata
dataLogDir=/usr/zookeeper/apache-zookeeper-3.5.7-bin/zkdatalog
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888
```
#修改myid
[root@master conf]# echo 1 > ../zkdata/myid

#分发并修改myid
[root@master conf]# cd ../../../
[root@master usr]# scp -r zookeeper/ slave1:$PWD
[root@master usr]# scp -r zookeeper/ slave2:$PWD
[root@master usr]# scp /etc/profile slave1:/etc
[root@master usr]# scp /etc/profile slave2:/etc

[root@slave1 /]# source /etc/profile
[root@slave1 /]# cd /usr/zookeeper/apache-zookeeper-3.5.7-bin/zkdata
[root@slave1 zkdata]# echo 2 > myid

[root@slave2 /]# source /etc/profile
[root@slave2 /]# cd /usr/zookeeper/apache-zookeeper-3.5.7-bin/zkdata
[root@slave2 zkdata]# echo 3 > myid
````
启动并查看状态
```bash
[root@master usr]# zkServer.sh start
[root@slave1 zkdata]# zkServer.sh start
[root@slave2 zkdata]# zkServer.sh start

[root@master usr]# jps
679 Jps
601 QuorumPeerMain

[root@master usr]# zkServer.sh status
[root@slave1 zkdata]# zkServer.sh status
[root@slave2 zkdata]# zkServer.sh status
````

### 3. Hadoop集群搭建

hadoop解压并配置环境变量

```bash

[root@master /]# mkdir -p /usr/hadoop
[root@master /]# cd /opt/software/
[root@master software]# tar -zxvf hadoop-2.7.7.tar.gz -C /usr/hadoop/

[root@master software]# echo "export HADOOP_HOME=/usr/hadoop/hadoop-2.7.7" >> /etc/profile
[root@master software]# echo "export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin" >> /etc/profile
[root@master software]# source /etc/profile

```

修改配置文件

```bash
[root@master software]# cd /usr/hadoop/hadoop-2.7.7/etc/hadoop
```

hadoop-env.sh

```sh
[root@master hadoop]# vim hadoop-env.sh
export JAVA_HOME=/usr/java/jdk1.8.0_212
```
yarn-env.sh

```sh
[root@master hadoop]# vim yarn-env.sh
export JAVA_HOME=/usr/java/jdk1.8.0_212
```
core-site.xml

```xml
[root@master hadoop]# vim core-site.xml
<configuration>
<!--配置hdfs连接地址master:9000-->
<property>
	<name>fs.default.name</name>
    <value>hdfs://master:9000</value>
</property> 
<!--用于配置临时存储目录/root/hadoopData/tmp-->
<property>
    <name>hadoop.tmp.dir</name>
    <value>/root/hadoopData/tmp</value>
</property>
</configuration>
```

hdfs-site.xml

```xml
[root@master hadoop]# vim hdfs-site.xml
<configuration>
  <!-- 设置备份文本数量为2 -->
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <!-- 指定NN存放元数据信息路径为本地 -->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/root/hadoopData/name</value>
    </property>

    <!-- 指定DN存放元数据信息路径为本地，要求绝对路径 -->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/root/hadoopData/data</value>
    </property>

    <!-- 关闭Hadoop集群权限校验，允许其他用户连接集群 -->
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>

    <!-- 指定Datanode之间通过域名方式进行通信 -->
    <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>true</value>
    </property>
</configuration>
```

mapred-site.xml

```xml
[root@master hadoop]# vim mapred-site.xml

<configuration>
<!--配置mr程序资源调度管理器为yarn-->
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>
</configuration>
```

yarn-site.xml

```xml
[root@master hadoop]# vim yarn-site.xml
<configuration>
<!--设置yarn主角色主机master:18141-->
<property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>master:18141</value>
</property>
<!--启用 MapReduce Shuffle 服务，确保 MapReduce 作业的正确执行-->
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
<property>
    <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
</configuration>
```

master

```sh
[root@master hadoop]# vim master
master

```



slaves

```bash
[root@master hadoop]# vim slaves
slave1
slave2
```

分发

```sh
[root@master hadoop]# cd /usr/
[root@master usr]# scp -r hadoop/ slave1:$PWD
[root@master usr]# scp -r hadoop/ slave2:$PWD
[root@master usr]# scp /etc/profile slave1:/etc
[root@master usr]# scp /etc/profile slave2:/etc

[root@slave1 /]# source /etc/profile
[root@slave2 /]# source /etc/profile
```

初始化并启动

```bash
[root@master usr]# hdfs namenode -format
[root@master usr]# start-all.sh
[root@master usr]# jps
3553 SecondaryNameNode
3362 NameNode
3970 Jps
3711 ResourceManager
[root@slave1 /]# jps
1330 Jps
1193 NodeManager
1085 DataNode
[root@slave2 /]# jps
947 NodeManager
840 DataNode
1096 Jps
```

## 模块D:Hive数据仓库



### 1. Hive基础环境配置

解压和环境变量

```bash
[root@master /]# mkdir -p /usr/hive
[root@master /]# cd /opt/software/
[root@master software]# tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /usr/hive/
[root@master software]# echo "export PATH:$PATH:$HIVE_HOME/bin" >> /etc/profile
[root@master software]# source /etc/profile
[root@master software]# cd /usr/hive/apache-hive-3.1.2-bin/conf/
```
配置

```bash
[root@master conf]# vim hive-env.sh
HADOOP_HOME=/usr/hadoop/hadoop-2.7.7
export HIVE_CONF_DIR=/usr/hive/apache-hive-3.1.2-bin/conf
export HIVE_AUX_JARS_PATH=/usr/hive/apache-hive-3.1.2-bin/lib

```
解决依赖问题

```bash
[root@master conf]# cd ../lib
[root@master lib]# cp /usr/hadoop/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar ./
[root@master lib]# cp jline-2.12.jar /usr/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/
[root@master lib]# cp /opt/software/mysql-connector-java-5.1.37.jar ./
[root@master lib]# rm -vrf guava-19.0.jar
```

### 2.配置HIVE元数据至mysql

hive-site.xml

```xml
[root@master conf]# cd ../conf
[root@master conf]# vim hive-site.xml
<configuration>
<!--配置元数据位置-->
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive_remote/warehouse</value>
</property>
<!--禁用本地元数据-->
<property>
  <name>hive.metastore.local</name>
  <value>false</value>
</property>
<!--使用slave1:9083端口-->
<property>
  <name>hive.metastore.uris</name>
  <value>thrift://slave1:9083</value>
</property>
<!--配置连接路径(自动创建数据库,禁用ssl),驱动名,mysql用户名,密码-->
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value>
</property>

<property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
</property>
<property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
</property>
<property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
</property>
</configuration>
```

### 3.配置Hive客户端
初始化
```bash
[root@master conf]# schematool -dbType mysql -initSchema
```

启动hive元数据

```bash
[root@master conf]# hive --service metastore
```

启动hive

```
hive	
```

## 模块E:Spark分析平台搭建

解压和环境变量

```bash
[root@master /]# mkdir -p /usr/spark
[root@master /]# cd /opt/software/
[root@master software]# tar -zxvf spark-3.1.1-bin-hadoop3.2.tgz -C /usr/spark/
[root@master software]# echo "export SPARK_HOME=/usr/spark/spark-3.1.1-bin-hadoop3.2" >> /etc/profile
[root@master software]# echo "export PATH=$PATH:$SPARK_HOME/bin" >> /etc/profile
[root@master software]# source /etc/profile
[root@master software]# cd /usr/spark/spark-3.1.1-bin-hadoop3.2/conf/
```

配置spark-env.sh

````bash
[root@master conf]# cp spark-env.sh.template spark-env.sh
[root@master conf]# vim spark-env.sh
```
#指定Master的IP
export SPARK_MASTER_HOST=master
#添加java环境变量
export JAVA_HOME=/usr/java/jdk1.8.0_212
#设置内存限制8g
export SPARK_WORKER_MEMORY=8g
#指定Master的端口
export SPARK_MASTER_PORT=7077
#设置Hadoop配置
export HADOOP_HOME=/usr/hadoop/hadoop-2.7.7
export HADOOP_CONF_DIR=/usr/hadoop/hadoop-2.7.7/etc/hadoop

```
````

配置slave

```bash
slave1
slave2
```

分发

```bash
[root@master usr]# scp -r spark/ slave1:$PWD
[root@master usr]# scp -r spark/ slave2:$PWD
[root@master usr]# scp /etc/profile slave1:/etc
[root@master usr]# scp /etc/profile slave2:/etc

[root@slave1 /]# source /etc/profile
[root@slave2 /]# source /etc/profile
```

启动spark并查看进程

```bash
[root@master usr]# cd $SPARK_HOME/sbin
[root@master sbin]# ./start-all.sh

jps进程分布：
master：Master
slave1：Worker
slave2：Worker
```



## 模块F:大数据集群应用与数据分析

准备sonnet.txt

```bash
[root@master /]# cd usr/hadoop/hadoop-2.7.7/
[root@master hadoop-2.7.7]# vim sonnet.txt
alibaba alibaba
tengxun
alibaba
same same
come alibaba
```

上传到hdfs

```bash
[root@master hadoop-2.7.7]# hdfs dfs -mkdir /input
[root@master hadoop-2.7.7]# hdfs dfs -put sonnet.txt /input/
[root@master hadoop-2.7.7]# hdfs dfs -ls /input/
```

词频统计

```bash
[root@master hadoop-2.7.7]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /input /output
[root@master hadoop-2.7.7]# hdfs dfs -ls /output/
```



