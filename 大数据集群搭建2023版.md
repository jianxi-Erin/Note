# **大数据集群搭建**

---

本篇⽂章所有组件安装包均在/opt/software⽬录下

安装⽬录在/opt/module    						   ` 如目录不存在请提前创建`

由于是容器环境，不能修改静态主机名，可以使用hostname [主机名]方式临时修改，使用bash命令初始化主机名

列如：

```shell
[root@bigdata1 ~]# hostname master                      #如使用虚拟机可hostnamectl set-hostname master
[root@bigdata1 ~]# bash
[root@master ~]# 
```

将三台主机名分别修改为master,slave1,slave2

#### `√` 一：配置免密登录

###### 1.打开/etc/hosts文件，添加ip映射

```shell
[root@master /]# vi /etc/hosts
```

在文件底部添加ip映射，ip为集群主机的真实ip，可使用 ip addr 命令查看

```shell
192.168.XXX.XXX		master
192.168.XXX.XXX		slave1
192.168.XXX.XXX		slave2
#XXX为三台主机的ip
```

分别在slave1、slave2上也进行添加

###### 2.生成本机密钥并分发至集群

```shell
[root@master /]# ssh-keygen -t rsa
#一直回车
#将密钥分发至所有主机
[root@master /]# ssh-copy-id master
[root@master /]# ssh-copy-id slave1
[root@master /]# ssh-copy-id slave2
```

分别在slave1、slave2上也执行上述命令



###### 3.测试是否免密成功

```shell
[root@master /]# ssh slave1
Last login: Tue Mar 28 20:28:14 2023 from 192.168.10.1
[root@slave1 ~]# 
```

#### `√` 二：Hadoop完全分布式部署

**Hadoop是一个开源的分布式计算框架，用于存储和处理大规模数据集。由hdfs（存储），mapreduce（计算），yarn（资源调度）**


要求准备好基础环境:包括但不限于:ssh免密,主机与IP的映射,修改主机名等

###### 1.解压Hadoop

进入/opt/software目录下

```shell
[root@master software]# tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
[root@master software]# tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
```

解压完成后，进入/opt/module目录下，修改目录名（便于配置路径）

```shell
[root@master module]# mv jdk1.8.0_212/ jdk
[root@master module]# mv hadoop-3.1.3/ hadoop
```

###### 2.配置环境变量

进入/etc/profile文件

```shell
[root@master module]# vi /etc/profile
```

在文件底部添加如下

```shell
export JAVA_HOME=/opt/module/jdk
export HADOOP_HOME=/opt/module/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin


```

保存后，需要执行source命令

```shell
[root@master module]# source /etc/profile
```

测试环境变量是否配置成功



```shell
[root@master module]# java -version
java version "1.8.0_212"
Java(TM) SE Runtime Environment (build 1.8.0_212-b10)
Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)
[root@master module]# hadoop version
Hadoop 3.1.3
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579
Compiled by ztang on 2019-09-12T02:47Z
Compiled with protoc 2.5.0
From source with checksum ec785077c385118ac91aadde5ec9799
This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar
[root@master module]# 
```

###### 3.修改配置文件

进入/opt/module/hadoop/etc/hadoop目录

修改hadoop-env.sh

```sh
export JAVA_HOME=/opt/module/jdk
#hadoop3.x必须配置
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

修改core-site.xml

```xml
<configuration>
<!--配置hdfs连接地址-->
<property>
	<name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
</property> 
<!--用于配置日志数据目录,可省略,默认值/tmp/hadoop*-->
<property>
    <name>hadoop.tmp.dir</name>
    <value>/opt/module/hadoop/tmp</value>
</property>
</configuration>
```

修改hdfs-site.xml

```xml
<configuration>
<!--配置数据副本数,可省略,默认值3-->
<property>
	<name>dfs.replication</name>    
    <value>3</value>
</property>
<!--配置secondary namenode连接地址,辅助namenode管理-->
<property>
	<name>dfs.namenode.secondary.http-address</name>    
    <value>slave1:50090</value>
</property>
</configuration>
```

修改mapred-site.xml

```xml
<configuration>
<!--配置mr程序资源调度管理器为yarn-->
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>
<!--配置hadoop classpath,运行mr程序需配置,否则可省略-->
<property>        
 <name>mapreduce.application.classpath</name>	  <value>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>
</property>
</configuration>
```

修改yarn-site.xml

```xml
<configuration>
<!--设置yarn主角色主机-->
<property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
</property>
<!--启用 MapReduce Shuffle 服务，确保 MapReduce 作业的正确执行-->
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
</configuration>
```

修改workers                                           

```sh
#datanode工作机器
master
slave1
slave2
```

###### 4.分发集群

```shell
#分发jdk
[root@master /]# scp -r /opt/module/jdk slave1:/opt/module/
[root@master /]# scp -r /opt/module/jdk slave2:/opt/module/
#分发hadoop
[root@master /]# scp -r /opt/module/hadoop/ slave1:/opt/module/
[root@master /]# scp -r /opt/module/hadoop/ slave2:/opt/module/
#分发环境变量
[root@master /]# scp /etc/profile slave1:/etc/profile
[root@master /]# scp /etc/profile slave2:/etc/profile
#注意：环境变量分发完后，需要在对应主机上执行 source /etc/profile 命令，否则环境变量不生效。
```



###### 5.格式化hdfs文件系统

```shell
[root@master /]# hdfs namenode -format
```

查看格式化最后结果，出现 **successfully formatted** 等字符，即为成功。

######  6.启动hadoop

第一种方式：单独启动hdfs和yarn

```shell
[root@master /]# start-dfs.sh 
[root@master /]# start-yarn.sh 
```

第二种方式：全部启动

```shell
[root@master /]# start-all.sh 
```

分别在三台主机使用jps命令查看进程

```shell
[root@master /]# jps
```

以下是所有的进程分布：

```
master：NameNode、DataNode、ResourceManager、NodeManager

slave1：SecondaryNamenode、DataNode、NodeManager

slave2：DataNode、NodeManager
```

注意：如果hdfs格式化成功后，因错误问题要再次格式化hdfs系统，需要把集群所有主机hadoop目录下的tmp和logs目录删除才能再次格式化。

验证mr程序,在master执行

```shell
[root@master /]#hadoop jar /opt/module/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 5

#执行结果
...
Estimated value of Pi is 3.68000000000000000000
...
```



#### `√`  三：Zookeeper安装配置

ZooKeeper是一个开源的分布式协调服务，用于实现分布式应用程序的一致性、可靠性和协调管理。**监听集群状态，实现HA**

前置环境:jdk已正确安装

###### 1.解压Zookeeper

进入/opt/software目录

```shell
[root@master software]# tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/
```

解压完成后，进入/opt/module目录下，修改目录名（便于配置路径）

```shell
[root@master module]# mv apache-zookeeper-3.5.7-bin/ zookeeper
```

###### 2.配置环境变量

进入/etc/profile文件,在文件中增加ZK_HOME环境变量

```shell
export JAVA_HOME=/opt/module/jdk
export HADOOP_HOME=/opt/module/hadoop
export ZK_HOME=/opt/module/zookeeper
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin
```

保存后，需要执行source命令	

```shell
[root@master module]# source /etc/profile
```

###### 3.修改配置文件

进入/opt/module/zookeeper/conf目录

复制zoo_sample.cfg文件 并命名为 zoo.cfg

```shell
[root@master conf]# cp zoo_sample.cfg zoo.cfg
```

修改zoo.cfg文件

```shell
#修改data存储目录
dataDir=/opt/data/zookeeper/zkdata
#在文件底部添加
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888
```

###### 4.创建myid文件

由于/opt/data/zookeeper/zkdata目录并没有创建，所以要新建此目录

```shell
[root@master /]# mkdir -p /opt/data/zookeeper/zkdata
```

进入/opt/data/zookeeper/zkdata目录，执行以下命令

```shell
[root@master zkdata]# echo 1 > myid
```

###### 5.分发集群

```shell
#将zookeeper分发至集群
[root@master /]# scp -r /opt/module/zookeeper/ slave1:/opt/module/
[root@master /]# scp -r /opt/module/zookeeper/ slave2:/opt/module/
#将环境变量分发至集群
[root@master /]# scp /etc/profile slave1:/etc/profile
[root@master /]# scp /etc/profile slave2:/etc/profile
#注意：环境变量分发完后，需要在对应主机上执行 source /etc/profile 命令，否则环境变量不生效。
#将myid分发至集群
[root@master /]# scp -r /opt/data/ slave1:/opt/
[root@master /]# scp -r /opt/data/ slave2:/opt/
#注意：分别在slave1、slave2修改myid的内容为 2、3
```

###### 6.启动Zookeeper

```shell
[root@master /]# zkServer.sh start
#分别在三台主机上执行上述命令
#切记检查防火墙
```

###### 7.查看Zookeeper状态

```shell
#至少启动两台的zookeeper才可以查看状态
#可以在三台主机上执行以下命令查看状态
[root@master /]# zkServer.sh status            
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost.
Mode: follower
[root@master /]# 
```

#### `√` 四：Flume安装配置

Flume是一个开源的分布式**日志收集系统**。它旨在可靠地、高效地收集、聚合和传输大规模的日志数据。

前置环境:jdk,hadoop均正确安装

###### 1.解压Flume

进入/opt/software目录

```shell
[root@master software]# tar -zxvf apache-flume-1.9.0-bin.tar.gz -C /opt/module/
```

解压完成后，进入/opt/module目录下，修改目录名（便于配置路径）

###### 2.配置环境变量

进入/etc/profile文件,在文件中增加FLUME_HOME环境变量

```shell
export JAVA_HOME=/opt/module/jdk
export HADOOP_HOME=/opt/module/hadoop
export ZK_HOME=/opt/module/zookeeper
export FLUME_HOME=/opt/module/flume
#注意是在PATH变量后边追加，此处并不是换行
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$FLUME_HOME/bin
```

保存后，需要执行source命令

```shell
[root@master module]# source /etc/profile
```

###### 3.修改配置文件

进入/opt/module/flume/conf目录

复制flume-env.sh.template并改名为flume-env.sh

修改flume-env.sh文件,修改如下：

```shell
export JAVA_HOME=/opt/module/jdk
```

###### 4.替换Jar包

进入/opt/module/flume/lib目录下，复制hadoop/share/hadoop/common/lib目录下的guava到此目录。

```shell
[root@master lib]# cp /opt/module/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar ./
#将原本的guava删除
[root@master lib]# rm -rf guava-11.0.2.jar
```

###### 5.编写日志收集方案

用flume收集hadoop的NameNode、DataNode日志

在/opt/module/flume/conf目录下新建hdfs-log.conf文件,编写如下内容：

```shell
a1.sources = r1
a1.channels = c1
a1.sinks = k1

# definition
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups=f1 f2
a1.sources.r1.filegroups.f1=/opt/module/hadoop/logs/.*namenode.*log
a1.sources.r1.filegroups.f2=/opt/module/hadoop/logs/.*datanode.*log
#a1.sources.r1.fileHeader=true

#a1.sources.r1.positionFile=/opt/module/flume/data/taildir_position1.json
#a1.sources.r1.headers.f1.headerKey1=namenode
#a1.sources.r1.headers.f2.headerKey1=datanode




# channel
a1.channels.c1.type = memory
a1.sources.r1.channels = c1
a1.sinks.k1.channel=c1


#sinks
a1.sinks.k1.type=hdfs
a1.sinks.k1.hdfs.path=hdfs://master:9000/tmp/flume
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.rollInterval=1
a1.sinks.k1.hdfs.rollSize=1
a1.sinks.k1.hdfs.rollCount=1

```

###### 6.启动Flume

进入到/opt/module/flume/目录下

启动命令为：

```shell
flume-ng agent -c conf/ -f conf/hdfs-log.conf -n a1 -Dflume.root.logger=INFO,console
```

复制master的会话窗口

使用 echo命令向NameNode或DataNode写入信息

```shell
echo ccc >>/opt/module/hadoop/logs/hadoop-root-namenode.log
echo ccc >>/opt/module/hadoop/logs/hadoop-root-datanode.log
```

查看Flume是否真确收集。

```shell
hdfs dfs -ls /tmp/flume.....
```



#### `√` 五：Hive的安装配置

Hive是一个开源的数据仓库工具。它提供了**类似于SQL的查询语言（HiveQL）**，使用户可以通过类似于**关系型数据库**的方式进行数据的存储、查询和分析。

前置环境:jdk,hadoop,mysql均正确安装

###### 1.解压Hive

进入/opt/software目录,将hive解压到/opt/module目录下，并改名为hive

###### 2.配置环境变量

在/etc/profile文件添加

```shell
export HIVE_HOME=/opt/module/hive
#在PATH变量末尾追加,此处并不是换行
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$FLUME_HOME/bin:$HIVE_HOME/bin
```

别忘记执行source命令

###### 3.修改配置文件

进入/opt/module/hive/目录

修改hive-env.sh 

```shell
HADOOP_HOME=/opt/module/hadoop
```

新建hive-site.xml文件，并添加内容如下

```xml
<configuration>
<property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
</property>
<property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
</property>
<property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
</property>
<property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
</property>
</configuration>
```

###### 4.替换并添加包

进入/opt/module/hive/lib/目录

将hadoop里的guava包替换到此处

```shell
[root@master lib]# cp /opt/module/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar ./
#将原本的guava删除
[root@master lib]# rm -rf guava-19.0.jar
```

将mysql的驱动包复制到此处

```shell
[root@master lib]# cp /opt/software/mysql-connector-java-5.1.37.jar ./
```

###### 5.初始化

执行命令

```shell
[root@master lib]# schematool -dbType mysql -initSchema
```

结果为：

```shell
Initialization script completed
schemaTool completed
```

使用hive命令，即可进入命令行。

###### 6.处理log4j版本冲突

如果进入log4j或者执行查询出现过多日志信息导致影响正常使用

进入/opt/module/hive/conf

```shell
vim log4j.properties

#wq保存退出
```



#### `√` 六：Kafka安装配置

Kafka是一个开源的分布式流处理平台。它提供了高性能、可扩展的**消息传递系统**，用于**处理实时流数据的发布、订阅和处理**。

前置环境:jdk,hadoop,zookeeper均正确安装

###### 1.解压Kafka

进入/opt/software目录,将kafka解压到/opt/module目录下，并改名为kafka

###### 2.配置环境变量

在/etc/profile文件添加

```shell
export KAFKA_HOME=/opt/module/kafka
#在PATH变量末尾追加
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$FLUME_HOME/bin:$HIVE_HOME/bin:$KAFKA_HOME/bin
```

别忘记执行source命令

###### 3.修改配置文件

进入/opt/module/kafka/config目录

修改server.properties文件

```shell
broker.id=0
zookeeper.connect=master:2181,slave1:2181,slave2:2181
delete.topic.enable=true
host.name=master
```

###### 4.分发集群

```shell
#将zookeeper分发至集群
[root@master /]# scp -r /opt/module/kafka/ slave1:/opt/module/
[root@master /]# scp -r /opt/module/kafka/ slave2:/opt/module/
#将环境变量分发至集群
[root@master /]# scp /etc/profile slave1:/etc/profile
[root@master /]# scp /etc/profile slave2:/etc/profile
#注意：环境变量分发完后，需要在对应主机上执行 source /etc/profile 命令，否则环境变量不生效。
```

注意：分发完后，分别修改对应主机server.properties文件，修改如下

slave1：

```shell
broker.id=1
zookeeper.connect=master:2181,slave1:2181,slave2:2181
delete.topic.enable=true
host.name=slave1
```

slave2：

```shell
broker.id=2
zookeeper.connect=master:2181,slave1:2181,slave2:2181
delete.topic.enable=true
host.name=slave2
```

###### 5.启动kafka

分别在三台主机执行：

```shell
[root@master kafka]# kafka-server-start.sh -daemon config/server.properties

#创建主题(端口号为2181可以省略)
kafka-topics.sh --create --topic installtopic --partitions 2 --replication-factor 2 --zookeeper master:2181,slave1:2181,slave2:2181
#列出主题(端口号为2181可以省略)
kafka-topics.sh --list --zookeeper master:2181,slave1:2181,slave2:2181
#删除主题 (端口号为2181可以省略)
kafka-topic.sh --delete --topic installtopic --zookeeper master:2181,slave1:2181,slave2:2181
#创建数据生产者
kafka-console-producer.sh --broker-list master:9092,slave1:9092,slave2:9092 --topic my-topic
#数据消费者 保证和生产者的端口号和topic一致         最大消费数据条数max-messages 2
kafka-console-consumer.sh --bootstrap-server master:9092,slave1:9092,slave2:9092 --topic ods_mall_log
```

注意：server.properties文件所在路径

使用jps命令查看进程



#### `√` 七：Hbase分布式安装配置

HBase是一个开源的分布式、可扩展的**列式数据库**，**建立在Hadoop分布式文件系统（HDFS）之上**。它被设计为具有高容错性、高可靠性和高性能的非关系型数据库解决方案。

前置环境:jdk,hadoop,zookeeper均正确安装



###### 1.解压Hbase

进入/opt/software目录,将hbase解压到/opt/module目录下，并改名为hbase

###### 2.配置环境变量

在/etc/profile文件添加

```shell
export HBASE_HOME=/opt/module/hbase
#在PATH变量末尾追加
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$FLUME_HOME/bin:$HIVE_HOME/bin:$KAFKA_HOME/bin:$HBASE_HOME/bin
```

别忘记执行source命令

###### 3.修改配置文件

进入/opt/module/hbase/conf/目录

修改hbase-env.sh文件

```shell
 export JAVA_HOME=/opt/module/jdk
 HBASE_MANAGES_ZK=false
```

修改hbase-site.xml文件

```xml
<configuration>
<property>
        <name>hbase.rootdir</name>
        <value>hdfs://master:9000/hbase</value>
</property>

<property>
        <name>hbase.zookeeper.quorum</name>
        <value>master:2181,slave1:2181,slave2:2181</value>
</property>
<property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
</property>
<property>
        <name>hbase.unsafe.stream.capability.enforce</name>
        <value>false</value>
</property>
</configuration>
```

修改regionservers文件

```shell
slave1
slave2
```

新增backup-masters文件(防止单点故障而备用的主节点)，添加如下内容

```
slave1
```

###### 4.分发集群

```shell
#将zookeeper分发至集群
[root@master /]# scp -r /opt/module/hbase/ slave1:/opt/module/
[root@master /]# scp -r /opt/module/hbase/ slave2:/opt/module/
#将环境变量分发至集群
[root@master /]# scp /etc/profile slave1:/etc/profile
[root@master /]# scp /etc/profile slave2:/etc/profile
#注意：环境变量分发完后，需要在对应主机上执行 source /etc/profile 命令，否则环境变量不生效。
```

###### 5.启动Hbase

在master主机运行以下命令：

```shell
[root@master /]# start-hbase.sh
```

使用jps命令查看进程

进程分布：

```shell
master：HMaster

slave1：HMaster、HRegionServer

slave2：HRegionServer
```

###### 6查看hbase命名空间

进入hbase shell

```shell
hbase shell
```

查看命名空间

```
list_namespace
```



#### `√`八：Spark安装配置

Spark（Apache Spark）是一个快速、通用的**大数据处理和分析引擎**，旨在处理大规模数据集的并行计算任务。它提供了高性能、易用的编程接口和丰富的工具，能够处理包括批处理、流处理、机器学习和图计算等多种数据处理任务。

前置环境:jdk,hadoop均正确安装

###### 1.解压spark

进入/opt/software目录,将spark解压到/opt/module目录下，并改名为spark

###### 2.配置环境变量

在/etc/profile文件添加

```shell
export SPARK_HOME=/opt/module/spark
#在PATH变量末尾追加
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$FLUME_HOME/bin:$HIVE_HOME/bin:$KAFKA_HOME/bin:$HBASE_HOME/bin:$SPARK_HOME/bin
```

别忘记执行source命令

###### 3.修改配置文件

进入/opt/module/spark/conf/目录

修改spark-env.sh文件（先复制spark-env.sh.template再改名）

```shell
#添加java环境变量
export JAVA_HOME=/opt/module/jdk
#指定Master的IP
export SPARK_MASTER_HOST=master
#指定Master的端口
export SPARK_MASTER_PORT=7077

```

修改workers文件（先复制workers.template再改名）

```shell
slave1
slave2
```

###### 4.分发集群

```shell
#将zookeeper分发至集群
[root@master /]# scp -r /opt/module/spark/ slave1:/opt/module/
[root@master /]# scp -r /opt/module/spark/ slave2:/opt/module/
#将环境变量分发至集群
[root@master /]# scp /etc/profile slave1:/etc/profile
[root@master /]# scp /etc/profile slave2:/etc/profile
#注意：环境变量分发完后，需要在对应主机上执行 source /etc/profile 命令，否则环境变量不生效。
```

###### 5.启动Spark

启动命令为：start-all.sh

因为命令与hadoop的命令重复，所以只能在/opt/module/spark/sbin/目录下使用：

```shell
[root@master bin]# ./start-all.sh
```

进程分布：

master：Master

slave1：Worker

slave2：Worker

###### 6.Spark on yarn模式

进入/opt/module/hadoop/etc/hadoop/目录下：

修改yarn-site.xml文件：

```xml
<!--在configuration标签里追加如下内容-->
<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>
<property>
	<name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
```

修改完后需要分发集群，

再次修改spark-env.sh文件

```shell
#添加java环境变量
export JAVA_HOME=/opt/module/jdk

#将这两个注释掉
#export SPARK_MASTER_HOST=master
#export SPARK_MASTER_PORT=7077

#添加yarn的配置文件路径
export YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop
```

修改后分发集群

注意：重启yarn或hadoop集群 ，yarn模式不需要启动spark

测试命令：

```shell
spark-submit --master yarn --class org.apache.spark.examples.SparkPi  $SPARK_HOME/examples/jars/spark-examples_2.12-3.1.1.jar
```

（jar包版本可自行切换）

如需进入web ui

可以使用master:8080,但默认端口容易被占用

```shell
方法一:#spark-env.sh,添加SPARK_MASTER_WEBUI_PORT=4040

方法二:#sbin/start-master.sh,修改默认端口SPARK_MASTER_WEBUI_PORT=4040
```



#### `√` 九：Flink安装配置

Flink（Apache Flink）是一个开源的**流处理和批处理框架**，旨在处理大规模、高吞吐量的实时数据流。它提供了高性能、可伸缩和容错的数据处理能力，同时支持事件时间处理和状态管理。

前置环境:jdk,hadoop均正确安装



###### 1.解压flink

进入/opt/software目录,将flink解压到/opt/module目录下，并改名为flink

###### 2.配置环境变量

在/etc/profile文件添加



```shell
export FLINK_HOME=/opt/module/flink
#在PATH变量末尾追加
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$FLUME_HOME/bin:$HIVE_HOME/bin:$KAFKA_HOME/bin:$HBASE_HOME/bin:$SPARK_HOME/bin:$FLINK_HOME/bin
```

别忘记执行source命令

###### 3.修改配置文件

进入/opt/module/flink/conf/目录

修改flink-conf.yaml文件

```yaml
jobmanager.rpc.address: master
#在末尾添加(冒号：后面含有空格)
classloader.check-leaked-classloader: false
```

修改workers文件

```shell
slave1
slave2
```

###### 4.分发集群

```shell
#将zookeeper分发至集群
[root@master /]# scp -r /opt/module/flink/ slave1:/opt/module/
[root@master /]# scp -r /opt/module/flink/ slave2:/opt/module/
#将环境变量分发至集群
[root@master /]# scp /etc/profile slave1:/etc/profile
[root@master /]# scp /etc/profile slave2:/etc/profile
#注意：环境变量分发完后，需要在对应主机上执行 source /etc/profile 命令，否则环境变量不生效。
```

###### 5.启动Flink

在master主机执行命令：start-cluster.sh 

进程分布：

```shell
master：StandaloneSessionClusterEntrypoint             

slave1：taskManagerRunner

slave2：taskManagerRunner
```



###### 6.Flink on yarn模式

在/etc/profile文件添加如下内容：

```shell
#注意要在PATH变量下边追加
export HADOOP_CLASSPATH=`hadoop classpath`
#export HADOOP_CONF_DIR=/opt/module/hadoop/etc/hadoop
```

重启hadoop后，测试：

```shell
[root@master /]# flink run -m yarn-cluster $FLINK_HOME/examples/batch/WordCount.jar
```

#### `√` 十：Clickhouse单机安装配置

ClickHouse是一个开源的列式数据库管理系统，主要用于快速和可伸缩的大数据分析。它以高性能、低延迟和高吞吐量为特点。**支持SQL**

前置环境:jdk,hadoop均正确安装

###### 1.解压Clickhouse

首先在/opt/module/目录下创建一个名为clickhouse文件夹

进入/opt/software/目录，分别解压：

```shell
[root@master software]# tar -zxvf clickhouse-common-static-21.9.4.35.tgz -C /opt/module/clickhouse
[root@master software]# tar -zxvf clickhouse-common-static-dbg-21.9.4.35.tgz -C /opt/module/clickhouse
[root@master software]# tar -zxvf clickhouse-server-21.9.4.35.tgz -C /opt/module/clickhouse
[root@master software]# tar -zxfv clickhouse-client-21.9.4.35.tgz -C /opt/module/clickhouse
```

###### 2.执行脚本

分别执行：

```shell
#执行顺序固定
[root@master clickhouse]# ./clickhouse-common-static-21.9.4.35/install/doinst.sh 
[root@master clickhouse]# ./clickhouse-common-static-dbg-21.9.4.35/install/doinst.sh
#执行此条命令会设置客户端连接密码,然后输入y允许网络连接
[root@master clickhouse]# ./clickhouse-server-21.9.4.35/install/doinst.sh
[root@master clickhouse]# ./clickhouse-client-21.9.4.35/install/doinst.sh 
```

###### 3.修改配置文件

进入/etc/clickhouse-server/目录

修改config.xml文件

```xml
<!--将clickhouse连接的端口改为9001,避免和hadoop冲突-->
<tcp_port>9001</tcp_port> 
<!--接触下列的注释-->
<listen_host>::</listen_host>
<!--<listen_host>0.0.0.0</listen_host>-->
```

再进入当前目录config.d/目录

删除listen.xml文件

```bash
rm -vrf /etc/clickhouse-server/config.d/listen.xml
```

###### 4.启动Clickhouse

启动命令为：systemctl start clickhouse-server

查看状态：systemctl status clickhouse-server

状态为：active (running) 为启动正常

###### 5.客户端连接

连接命令为：

```shell
clickhouse-client --password [自己设置的密码] --port 9001
```

###### 6.忘记default密码

修改/etc/clickhouse-server/users.d/default-password.xml

```xml
<!--删除password_sha256_hex标签,然后修改password标签-->
<password>你的新密码</password>
```

重启clickhouse server 

```bash
systemctl restart clickhouse-server
```

用新密码连接

```bash
clickhouse-client --password [新密码] --port 9001
```

#### `√` 十一:Hudi部署

Hudi（Hadoop Upserts Deletes and Incrementals）是一个开源的数据湖解决方案，旨在提供高效、可靠和可伸缩的数据管理和处理能力。它是基于Apache Hadoop生态系统构建的，可以与Apache Spark、Apache Hive等大数据框架集成使用。

前置环境:jdk,hadoop,spark均正确安装

###### 1.解压maven

进入/opt/software/目录,将maven解压到/opt/module,并改名为maven

```shell
[root@master software]#tar -zxvf /opt/software/apache-maven-3.6.3-bin.tar.gz -C /opt/module/
```

###### 2.配置环境变量

在/etc/profile文件添加

```sh
export MAVEN_HOME=/opt/module/maven
#追加
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$FLUME_HOME/bin:$HIVE_HOME/bin:$KAFKA_HOME/bin:$HBASE_HOME/bin:$SPARK_HOME/bin:$FLINK_HOME/bin:$MAVEN_HOME/bin
#使用mvn -v验证
```

###### 3.配置maven本地库

编辑/opt/module/maven/conf/settings.xml

取消注释并修改

```xml
<localRepository>/opt/software/RepMaven/</localRepository>
        <mirror>
            <id>alimaven</id>
            <name>aliyun maven</name>
            <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
            <mirrorOf>central</mirrorOf>
        </mirror>

```

###### 4.解压hudi包

```shell
[root@master software]tar -zxvf /opt/software/hudi-0.11.0.src.tgz -C /opt/module/
```

###### 5. 集成hudi到spark

```sh
cp /opt/module/hudi-0.11.0/packaging/hudi-spark-bundle/target/hudi-spark3.1-bundle_2.12-0.11.0.jar /opt/module/spark/jars/
```

 ###### 6. 集成hudi到hive数据同步

   ```
   cp /opt/module/hudi-0.11.0/packaging/hudi-hadoop-mr-bundle/target/hudi-hadoop-mr-bundle-0.11.0.jar /opt/module/hive/lib/
   
   cp /opt/module/hudi-0.11.0/packaging/hudi-hive-sync-bundle/target/hudi-hive-sync-bundle-0.11.0.jar /opt/module/hive/lib/
   
   ```



###### 7.使用指定hudi配置进入spark shell

```shell
#hive元数据
nohup hive --service metastore & 
nohub hive --service hiveserver2 &
#spark-shell 3.1.1
spark-shell --master local[*] --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'

# For Spark versions: 3.0 - 3.1
spark-shell --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'
```

###### 8.输入以下代码测试(spark shell)

```scala
import org.apache.hudi.QuickstartUtils._
import scala.collection.JavaConversions._
import org.apache.spark.sql.SaveMode._
import org.apache.hudi.DataSourceReadOptions._
import org.apache.hudi.DataSourceWriteOptions._
import org.apache.hudi.config.HoodieWriteConfig._
import org.apache.hudi.common.model.HoodieRecord

val tableName = "testtable"
val basePath = "/tmp/hudi/"+tableName
val dataGen = new DataGenerator

val inserts = convertToStringList(dataGen.generateInserts(10))
val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))
df.write.format("hudi").
  options(getQuickstartWriteConfigs).
  option(PRECOMBINE_FIELD_OPT_KEY, "ts").
  option(RECORDKEY_FIELD_OPT_KEY, "uuid").
  option(PARTITIONPATH_FIELD_OPT_KEY, "uuid").
  option(TABLE_NAME, tableName).
 option("hoodie.datasource.hive_sync.enable", "true").
 option("hoodie.datasource.hive_sync.mode", "hms").
 option("hoodie.datasource.hive_sync.metastore.uris", "thrift://192.168.45.10:9083").
 option("hoodie.datasource.hive_sync.username", "root").
 option("hoodie.datasource.hive_sync.password", "123456").
 option("hoodie.datasource.hive_sync.database", "default").
 option("hoodie.datasource.hive_sync.table", tableName).
 option("hoodie.datasource.hive_sync.partition_fields", "uuid").
 option("hoodie.datasource.hive_sync.partition_extractor_class", "org.apache.hudi.hive.MultiPartKeysValueExtractor").
  mode(Overwrite).
  save(basePath)

val tripsSnapshotDF = spark.read.format("hudi").load(basePath)
tripsSnapshotDF.createOrReplaceTempView("testtable")
spark.sql("select fare, begin_lon, begin_lat, ts from  testtable where fare > 20.0").show()

```

###### 9. 使用指定Hudi配置进入spark sql

```sh

#spark-sql 3.1.1
spark-sql --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' 

# For Spark versions: 3.0 - 3.1
spark-sql --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'

#查询分区
show partitions default.testtable;

```

###### 10.使用idea编码

创建maven项目,并修改pom.xml文件

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.example</groupId>
    <artifactId>BigData</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <flink.version>1.14.0</flink.version>
        <scala.version>2.12.0</scala.version>
        <scala.binary.version>2.12</scala.binary.version>
        <hive.version>3.1.2</hive.version>
        <mysqlconnect.version>5.1.37</mysqlconnect.version>
        <clickhouse.version>0.3.2</clickhouse.version>
        <hdfs.version>3.1.3</hdfs.version>
        <spark.version>3.1.1</spark.version>
        <hbase.version>2.2.3</hbase.version>
        <kafka.version>2.4.1</kafka.version>
        <lang3.version>3.9</lang3.version>
        <flink-connector-redis.verion>1.1.5</flink-connector-redis.verion>
        <!--hudi版本控制-->
		
        <hudi.version>0.9.0</hudi.version>

    </properties>
    <dependencies>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-reflect</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-compiler</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <!-- kafka -->
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka_${scala.binary.version}</artifactId>
            <version>${kafka.version}</version>
        </dependency>
        <!-- flink  -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-runtime-web_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-scala_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-json</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-scala-bridge_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-redis_2.11</artifactId>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.flink</groupId>
                    <artifactId>flink-shaded-hadoop2</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.commons</groupId>
                    <artifactId>commons-lang3</artifactId>
                </exclusion>
            </exclusions>
            <version>${flink-connector-redis.verion}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-lang3</artifactId>
            <version>${lang3.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-hive_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-hbase-2.2_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>${mysqlconnect.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.hive</groupId>
                    <artifactId>hive-exec</artifactId>
                </exclusion>
            </exclusions>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.hive</groupId>
                    <artifactId>hive-exec</artifactId>
                </exclusion>
            </exclusions>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hdfs.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-auth</artifactId>
            <version>${hdfs.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-mapreduce</artifactId>
            <version>${hbase.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>${hbase.version}</version>
        </dependency>
        <dependency>
            <!--该pom文件依赖已过时-->
            <groupId>ru.yandex.clickhouse</groupId>
            <artifactId>clickhouse-jdbc</artifactId>
            <version>${clickhouse.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>com.fasterxml.jackson.core</groupId>
                    <artifactId>jackson-databind</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>net.jpountz.lz4</groupId>
                    <artifactId>lz4</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
 
        <!--hudi-->
        <dependency>
            <groupId>org.apache.hudi</groupId>
            <artifactId>hudi-spark3-bundle_${scala.binary.version}</artifactId>
            <version>${hudi.version}</version>
        </dependency>

    </dependencies>
    <build>
        <resources>
            <resource>
                <directory>src/main/scala</directory>
            </resource>
            <resource>
                <directory>src/main/java</directory>
            </resource>
            <resource>
                <directory>src/main/resources</directory>
            </resource>
        </resources>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <configuration>
                    <recompileMode>incremental</recompileMode>
                </configuration>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.1</version>
                <configuration>
                    <source>8</source>
                    <target>8</target>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>
```

配置项目所在主机的hosts

将集群的hdfs-site.xml , core-site.xml , hive-site.xml复制到项目resources下

编写offlineData.ds_db01.demo1.hudi_demo1_1.scala文件

```scala
package offlineData.ds_db01.demo1

import org.apache.hudi.DataSourceWriteOptions.{PARTITIONPATH_FIELD, PRECOMBINE_FIELD, RECORDKEY_FIELD, TABLE_NAME}
import org.apache.hudi.QuickstartUtils.getQuickstartWriteConfigs
import org.apache.hudi.config.HoodieWriteConfig.TBL_NAME
import org.apache.spark.sql.functions.{col, greatest, lit, when}
import org.apache.spark.sql.{SaveMode, SparkSession, functions}


object hudi_demo1_1 {
  /**
   * 虚拟表数据
   */
  def falseData(): Unit = {
    val mysqlURL = "jdbc:mysql://192.168.45.10:3306/ds_db01"
    val mysqlTable = "customer_inf"
    val mysqlUser = "root"
    val mysqlPassword = "123456"

    val hudiTable = "user_info"
    val hudiPath = "/user/hive/warehouse/ods_ds_hudi.db/" + hudiTable
    val hudiDatabase = "default"
    val partitionKey = "etl_date"

    val sc = SparkSession.builder()
      .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .config("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension")
      .master("local[*]")
      .enableHiveSupport()
      .getOrCreate()
    //    制造虚拟表数据
    sc.read.format("jdbc")
      .option("url", mysqlURL)
      .option("user", mysqlUser)
      .option("password", mysqlPassword)
      .option("dbtable", mysqlTable)
      .load()
      .limit(20)
      .withColumn("operate_time", col("modified_time"))
      .withColumn("create_time", col("register_time"))
      .withColumn("id", col("customer_inf_id"))
      .withColumn("etl_date", lit("20190919"))
      .limit(20)
      .write
      .format("hudi")
      .options(getQuickstartWriteConfigs)
      .option(RECORDKEY_FIELD.key(), "id")
      .option(PRECOMBINE_FIELD.key(), "operate_time")
      .option(PARTITIONPATH_FIELD.key(), partitionKey)
      .option(TBL_NAME.key(), hudiTable)
      .option("hoodie.datasource.hive_sync.enable", "true")
      .option("hoodie.datasource.hive_sync.mode", "hms")
      .option("hoodie.datasource.hive_sync.database", hudiDatabase)
      .option("hoodie.datasource.hive_sync.table", hudiTable)
      .option("hoodie.datasource.hive_sync.partition_fields", partitionKey)
      .option("hoodie.datasource.hive_sync.partition_extractor_class", "org.apache.hudi.hive.MultiPartKeysValueExtractor")
      .mode(SaveMode.Overwrite)
      .save(hudiPath)
    sc.stop()
    println("虚拟表创建中...")
  }

  def main(args: Array[String]): Unit = {
    falseData()

  }
}

```

进入spark-sql

```sql
spark-sql> show tables;
default	user_info	false

```



   

#### `√`十二：maxwell配置

Maxwell是一个开源的MySQL数据库binlog解析工具，用于**实时捕获MySQL数据库的变更，并将这些变更以结构化的形式输出到消息队列(例如kafka)或日志文件**。它可以帮助开发人员、数据工程师和数据分析师轻松地对数据库变更进行监控和处理。

前置环境:jdk,hadoop,zookeeper,kafka均正确安装

###### 1.解压maxwell

进入/opt/software目录,将maxwall解压到/opt/module目录下，并改名为maxwell

```shell
[root@master software]# tar -zxvf /opt/software/maxwell-1.29.2.tar.gz -C /opt/module/
[root@master software]# cd /opt/module/
[root@master module]# mv maxwell-1.29.2 maxwell

```

###### 2.配置环境变量

追加etc/profile,并source

```sh
export MAXWELL_HOME=/opt/module/maxwell
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$FLUME_HOME/bin:$HIVE_HOME/bin:$KAFKA_HOME/bin:$SPARK_HOME/bin:$FLINK_HOME/bini:$HBASE_HOME/bin:$MAVEN_HOME/bin:$MAXWELL_HOME/bin

```



###### 3.修改配置文件

进入/opt/module/maxwell目录

修改config.properties文件,在行首修改

```sh
producer=kafka
kafka.bootstrap.servers=master:9092,slave1:9092,slave2:9092
kafka_topic=ods_mall_data
# mysql login info
host=localhost
port=3306
user=root
password=123456
```

###### 4.修改mysql配置文件

修改/etc/my.cnf文件,在末尾追加

```sh
log-bin=/var/lib/mysql/mysql-bin
binlog-format=row
server_id=1

```

重启mysql服务

```shell
[root@master maxwell]# systemctl restart mysqld
```

###### 5.启动maxwall

确保zookeeper,kafka已启动且ods_mall_data主题存在

```shell
[root@master maxwell]# maxwell --config config.properties
```



#### `√` 十三: Hadoop HA高可用集群配置

高可用（High Availability，HA）是指系统能够持续提供高度可靠的服务，即使**在部分组件或节点发生故障的情况下也能保持正常运行**。HA是确保系统长时间运行、数据不丢失、业务持续可用的关键要素之一。

###### 1.准备环境

请先确保zookeeper,jdk,免密,IP映射等基础环境已配置完成

如已配置hadoop完全分布式请自行删除所有集群的hadoop`数据目录tmp`和`日志目录logs`

如准备重新配置,请先跟着`任务二`将`修改及分发环境变量`完成

###### 2.修改配置文件

进入/opt/module/hadoop/etc/hadoop目录

修改hadoop-env.sh

```sh
export JAVA_HOME=/opt/module/jdk
sexport HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
#以下为新增内容
export HDFS_JOURNALNODE_USER=root
export HDFS_ZKFC_USER=root
```

修改core-site.xml

```xml
<configuration>
<property>
        <name>fs.defaultFS</name>
        <value>hdfs://hdfscluster</value>
</property>
<!--设置日志,可省略-->
<property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop/tmp</value>
</property>
<!--设置zookeeper连接主机及端口-->
<property>
        <name>ha.zookeeper.quorum</name>
        <value>master:2181,slave1:2181,slave2:2181</value>
</property>

</configuration>

```

修改hdfs-site.xml

```xml
<configuration>
<!--设置副本数,可省略-->
<property>
	<name>dfs.replication</name>    
    <value>3</value>
</property>

<property>
        <name>dfs.nameservices</name>
        <value>hdfscluster</value>
</property>

<property>
        <name>dfs.ha.namenodes.hdfscluster</name>
        <value>master,slave1</value>
</property>

<property>
        <name>dfs.namenode.rpc-address.hdfscluster.master</name>
        <value>master:9000</value>
</property>
<property>
        <name>dfs.namenode.rpc-address.hdfscluster.slave1</name>
        <value>slave1:9000</value>
</property>

<property>
        <name>dfs.namenode.http-address.hdfscluster.master</name>
        <value>master:9870</value>
</property>
<property>
        <name>dfs.namenode.http-address.hdfscluster.slave1</name>
        <value>slave1:9870</value>
</property>
<property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://master:8485;slave1:8485;slave2:8485/hdfscluster</value>
</property>

<property>
		<name>dfs.client.failover.proxy.provider.hdfscluster</name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
<property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence
    shell(/bin/true)</value>
</property>
<property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
</property>
<property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
</property> 
 
</configuration>

```

修改 mapred-site.xml

```xml
<configuration>

<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
</property>
 <!--hadoop2.x无需配置 hadoop3.x必须配置,否则无法运行mr程序-->
<property>       
	<name>mapreduce.application.classpath</name>       										<value>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>
</property>
    
</configuration>

```

修改yarn-site.xml

```xml
<configuration>
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
    
<property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
</property>
<property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>yarncluster</value>
</property>

<property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
</property>

<property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>master</value>
</property>
<property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>slave1</value>
</property>
<property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>master:2181,slave1:2181,slave2:2181</value>
</property>
<property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
</property>
    
</configuration>
```

修改worker

```sh
master
slave1
slave2
```

###### 3.分发集群

使用scp命令将hadoop分发到其他集群

###### 4.初始化

启动zookeeper并检查状态(所有集群)

```shell
zkServer.sh start
zkServer.sh status     #两台及以上集群启动才能正常显示状态
```

启动jouurnalnode守护进程 (所有集群)

```shell
hdfs --daemon start journalnode						
```

格式化namenode (master)

```shell
hdfs namenode -format
```

格式化zkfc (master)

```shell
hdfs zkfc -formatZK
```

启动namenode(master)

```shell
hdfs --daemon start namenode
```

同步主master元数据(slave1) 

```shell
hdfs namenode -bootstrapStandby           #或者scp将master的hadoop数据发送到slave1
```

启动hdfs集群(master)

```shell
start-dfs.sh   #执行后master和slave1的namenode应均启动
```

启动yarn集群(master)

```shell
start-yarn.sh
```

###### 3.验证高可用

在三台集群使用jps检查进程

```shell
master:
	1778 JournalNode
	1651 QuorumPeerMain
	5219 ResourceManager
	5315 Jps
	2647 DataNode
	3528 NodeManager
	3017 DFSZKFailoverController
	2285 NameNode

slave1:
	3331 NameNode
	4548 Jps
	2437 DFSZKFailoverController
	2278 DataNode
	2024 JournalNode
	1915 QuorumPeerMain
	4155 ResourceManager
	2620 NodeManager

slave2:
	2002 NodeManager
	1605 QuorumPeerMain
	1739 JournalNode
	1837 DataNode
	2430 Jps



```

验证namenode自动切换

浏览器打开master:9870和slave1:9870

此时两台集群的状态为:

```shell
master:active
slave1:standby
```

使用kill命令杀掉master的namenode

```shell
kill -9 2285                 
```

浏览器刷新master:9870和slave1:9870

```shell
master:网页打不开
slave1:active
```

重启master的namenode

```shell
hdfs --daemon start namenode
```

浏览器刷新master:9870和slave1:9870

```
master:standby
slave1:active
```

至此,hadoop ha集群配置完成,下次启动ha仅需要先启动zookeeper然后start-all.sh



#### `√` 十四:Redis安装配置

Redis是基于**键值对**（key-value）的存储系统。它将数据存储在内存中，并使用键来唯一标识和访问相应的值。每个键都与一个特定的值相关联，这样可以通过键快速地获取对应的值。

###### 1.解压redis

进入/opt/software目录,将redis解压到/opt/module目录下

```bash
[root@master software]tar -zxvf redis-6.2.6.tar.gz -C /opt/module/
```

###### 2.安装依赖

```bash
[root@master redis-6.2.6]# yum -y install gcc-c++
```

###### 3.构建

进入/opt/module/redis-6.2.6/

```bash
[root@master redis-6.2.6]# make
[root@master redis-6.2.6]# make install
```

###### 4.修改配置文件

redis.conf

```sh
#bind 127.0.0.1 -::1
protected-mode no
```

###### 5.启动

```bash
#后台启动服务端
[root@master redis-6.2.6]# redis-server redis.conf --daemonize yes
#启动客户端
[root@master redis-6.2.6]# redis-cli

```

6.查看进程状态

```bash
[root@master redis-6.2.6]# netstat -nap |grep 6379
tcp        0      0 0.0.0.0:6379            0.0.0.0:*               LISTEN      6164/redis-server * 
tcp        0      0 127.0.0.1:57726         127.0.0.1:6379          TIME_WAIT   -                   
tcp6       0      0 :::6379                 :::*                    LISTEN      6164/redis-server * 

```

###### 7.设置或获取键值对

设置key

```bash
#设置key1的值为alue1
127.0.0.1:6379> set key1 value1
OK
```

得到key

```bash
#得到key1的值
127.0.0.1:6379> get key1
"value1"
```

###### 8.杀死进程

```bash
[root@master redis-6.2.6]# kill -9 6164
```



#### '√'十五.Azkaban安装配置

###### 1.上传软件包

```sh
#上传下列包到集群master /opt/software
azkaban-db-3.84.4.tar.gz
azkaban-exec-server-3.84.4.tar.gz
azkaban-web-server-3.84.4.tar.gz
```

###### 2.解压Azkaban

```sh
[root@bigdata1 ~]# cd /opt/software/
[root@bigdata1 module]# mkdir azkaban
[root@bigdata1 module]# cd /opt/software/
[root@bigdata1 software]# tar -zxvf azkaban-db-3.84.4.tar.gz -C /opt/module/azkaban/
[root@bigdata1 software]# tar -zxvf azkaban-web-server-3.84.4.tar.gz -C /opt/module/azkaban/
[root@bigdata1 software]# tar -zxvf azkaban-exec-server-3.84.4.tar.gz -C /opt/module/azkaban/

[root@bigdata1 ~]# cd /opt/module/azkaban/
[root@bigdata1 azkaban]# mv azkaban-db-3.84.4/ azkaban-db
[root@bigdata1 azkaban]# mv azkaban-web-server-3.84.4/ azkaban-web/
[root@bigdata1 azkaban]# mv azkaban-exec-server-3.84.4/ azkaban-exec/
```

###### 3.安装Mysql后,继续配置

```mysql
[root@bigdata1 azkaban]# mysql -uroot -p123456
mysql> create database azkaban;
Query OK, 1 row affected (0.04 sec)
#创建root用户远程登陆,并设置密码
mysql> use azkaban;
Database changed
#导入表
mysql> source /opt/module/azkaban/azkaban-db/create-all-sql-3.84.4.sql
...
mysql> quit;
Bye

```

修改mysql配置文件

```sh
[root@bigdata1 azkaban]# vim /etc/my.cnf

[mysqld]
max_allowed_packet=1024M

```

重启mysqld服务

```sh
[root@bigdata1 azkaban]#  systemctl restart mysqld;
```

###### 4.配置 Executor Server

```sh
[root@bigdata1 azkaban]# vim /opt/module/azkaban/azkaban-exec/conf/azkaban.properties 

```

修改配置

```properties
#...
default.timezone.id=Asia/Shanghai
#...
azkaban.webserver.url=http://master:8081
executor.port=12321
#...
database.type=mysql
mysql.port=3306
mysql.host=master
mysql.database=azkaban
mysql.user=root
mysql.password=123456
mysql.numconnections=100
```

分发配置

```sh
[root@bigdata1 module]# scp -r /opt/module/azkaban/ slave1:/opt/module/
[root@bigdata1 module]# scp -r /opt/module/azkaban/ slave2:/opt/module/
```

所有集群启动

```sh
[root@bigdata1 module]# cd /opt/module/azkaban/azkaban-exec/
[root@bigdata1 azkaban-exec]# bin/start-exec.sh 
```

所有集群激活executor

```sh
[root@bigdata1 azkaban-exec]# curl -G "master:12321/executor?action=activate" && echo
{"status":"success"} #即为成功
[root@bigdata2 azkaban-exec]# curl -G "bigdata1:12321/executor?action=activate" && echo
{"status":"success"}
[root@bigdata3 azkaban-exec]# curl -G "bigdata1:12321/executor?action=activate" && echo
{"status":"success"}

```

###### 5.配置 Web Server

修改azkaban.properties

```
[root@bigdata1 azkaban-exec]# vim /opt/module/azkaban/azkaban-web/conf/azkaban.properties 
```

```properties
...
default.timezone.id=Asia/Shanghai
...
database.type=mysql
mysql.port=3306
mysql.host=master
mysql.database=azkaban
mysql.user=root
mysql.password=123456
mysql.numconnections=100
...
azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus
#StaticRemainingFlowSize：正在排队的任务数；
#CpuStatus：CPU 占用情况
#MinimumFreeMemory：内存占用情况。测试环境，必须将 MinimumFreeMemory 删除掉，否则它会认为集群资源不够，不执行。
```



修改azkaban-users.xml 文件，添加 atguigu 用户

```sh
[root@bigdata1 azkaban-exec]# vim /opt/module/azkaban/azkaban-web/conf/azkaban-users.xml 
```

```xml
<azkaban-users>
  <user groups="root" password="123456" roles="admin" username="root"/>
  <user password="metrics" roles="metrics" username="metrics"/>
  <user password="atguigu" roles="admin" username="atguigu"/>
  <role name="admin" permissions="ADMIN"/>
  <role name="metrics" permissions="METRICS"/>
</azkaban-users>

```

启动web服务

```sh
[root@bigdata1 azkaban-exec]# cd /opt/module/azkaban/azkaban-web/
[root@bigdata1 azkaban-web]# bin/start-web.sh 
```

###### 6.登录Azkaban

```sh
打开浏览器
访问http://master:8081/
登录
```

